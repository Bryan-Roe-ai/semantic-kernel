@echo off
REM Local AI Quick Launch Script for Windows
REM
REM Copyright (c) 2025 Bryan Roe
REM Licensed under the MIT License
REM
REM This file is part of the Semantic Kernel - Advanced AI Development Framework.
REM Original work by Bryan Roe.
REM
REM Author: Bryan Roe
REM Created: 2025
REM License: MIT

setlocal EnableDelayedExpansion

REM Configuration
set "WORKSPACE_ROOT=%~dp0"
set "BACKEND_DIR=%WORKSPACE_ROOT%19-miscellaneous\src"
set "FRONTEND_DIR=%WORKSPACE_ROOT%07-resources\public"
set "LM_STUDIO_URL=http://localhost:1234"
set "BACKEND_URL=http://localhost:8000"

echo.
echo â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
echo â•‘                           ðŸš€ LOCAL AI QUICK LAUNCH ðŸš€                        â•‘
echo â•‘                        Semantic Kernel Local AI System                      â•‘
echo â•‘                             by Bryan Roe (2025)                            â•‘
echo â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
echo.

if "%1"=="stop" goto stop
if "%1"=="status" goto status
if "%1"=="help" goto help

:start
echo ðŸš€ Starting Local AI System...
echo.

echo ðŸ” Checking LM Studio...
curl -s %LM_STUDIO_URL%/v1/models >nul 2>&1
if %errorlevel% equ 0 (
    echo   âœ“ LM Studio is running
) else (
    echo   âœ— LM Studio is not running
    echo   ðŸ’¡ Please start LM Studio and enable the API server
    echo.
    pause
    goto end
)

echo.
echo ðŸ“¦ Installing dependencies...
pip install fastapi uvicorn requests pydantic starlette >nul 2>&1
if %errorlevel% equ 0 (
    echo   âœ“ Dependencies installed
) else (
    echo   âœ— Failed to install dependencies
    pause
    goto end
)

echo.
echo âš™ï¸ Setting up environment...

REM Create directories
if not exist "%BACKEND_DIR%\plugins" mkdir "%BACKEND_DIR%\plugins"
if not exist "%BACKEND_DIR%\uploads" mkdir "%BACKEND_DIR%\uploads"

REM Create .env file
echo LM_STUDIO_URL="%LM_STUDIO_URL%/v1/chat/completions" > "%BACKEND_DIR%\.env"
echo   âœ“ Environment configured

echo.
echo ðŸš€ Starting backend server...

REM Check if backend is already running
curl -s %BACKEND_URL%/ping >nul 2>&1
if %errorlevel% equ 0 (
    echo   â„¹ Backend server is already running
) else (
    REM Start backend
    cd /d "%BACKEND_DIR%"
    start /b python -m uvicorn backend:app --reload --host 127.0.0.1 --port 8000 > backend.log 2>&1

    echo   â³ Waiting for backend to start...
    timeout /t 5 /nobreak >nul

    curl -s %BACKEND_URL%/ping >nul 2>&1
    if %errorlevel% equ 0 (
        echo   âœ“ Backend server started successfully
    ) else (
        echo   âœ— Backend server failed to start
        echo   ðŸ“‹ Check backend.log for details
        pause
        goto end
    )
)

echo.
echo ðŸŒ Opening chat interface...
start "" "file:///%FRONTEND_DIR%/ai-chat-launcher.html"
echo   âœ“ Chat interface opened in browser

echo.
echo ðŸŽ‰ Local AI system is ready!
echo.
echo Available interfaces:
echo   â€¢ Advanced Chat: file:///%FRONTEND_DIR%/ai-chat-launcher.html
echo   â€¢ Plugin Chat:   file:///%FRONTEND_DIR%/plugin-chat.html
echo   â€¢ Simple Chat:   file:///%FRONTEND_DIR%/simple-chat.html
echo.
echo API Documentation: %BACKEND_URL%/docs
echo Backend Logs: %BACKEND_DIR%\backend.log
echo.
echo To stop the system, run: %~nx0 stop
echo.
pause
goto end

:stop
echo ðŸ›‘ Stopping backend server...
taskkill /f /im python.exe /fi "WINDOWTITLE eq *uvicorn*" >nul 2>&1
echo   âœ“ Backend processes stopped
pause
goto end

:status
echo System Status:
echo.

echo System Dependencies:
python --version >nul 2>&1
if %errorlevel% equ 0 (
    echo   âœ“ Python
) else (
    echo   âœ— Python (not found)
)

pip --version >nul 2>&1
if %errorlevel% equ 0 (
    echo   âœ“ pip
) else (
    echo   âœ— pip (not found)
)

echo.
echo AI Providers:
curl -s %LM_STUDIO_URL%/v1/models >nul 2>&1
if %errorlevel% equ 0 (
    echo   âœ“ LM Studio is running
) else (
    echo   âœ— LM Studio is not running
)

echo.
echo Backend Service:
curl -s %BACKEND_URL%/ping >nul 2>&1
if %errorlevel% equ 0 (
    echo   âœ“ Backend server is running
) else (
    echo   âœ— Backend server is not running
)

echo.
pause
goto end

:help
echo Usage: %~nx0 [COMMAND]
echo.
echo Commands:
echo   start     - Start the local AI system (default)
echo   stop      - Stop the backend server
echo   status    - Show system status
echo   help      - Show this help message
echo.
echo Examples:
echo   %~nx0 start    # Start everything and open chat interface
echo   %~nx0 stop     # Stop the backend server
echo   %~nx0 status   # Check if everything is working
echo.
pause
goto end

:end
endlocal
