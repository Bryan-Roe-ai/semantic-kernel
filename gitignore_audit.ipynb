{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1430bdba",
   "metadata": {},
   "source": [
    "# .gitignore Audit & Optimization Notebook\n",
    "\n",
    "This notebook analyzes the repository `.gitignore` for consistency, redundancy, and opportunities to consolidate patterns without changing intent.\n",
    "\n",
    "Outline implemented across the next cells:\n",
    "\n",
    "1. Load file\n",
    "2. Segment sections\n",
    "3. Normalize patterns\n",
    "4. Classify patterns\n",
    "5. Detect duplicates & shadowed\n",
    "6. Glob expansion (current ignored files)\n",
    "7. Redundant artifact patterns\n",
    "8. Ineffective negations\n",
    "9. Large tracked artifacts not ignored\n",
    "10. Interactive matcher\n",
    "11. Consolidation suggestions\n",
    "12. Draft generation\n",
    "13. Draft validation\n",
    "14. Export reports\n",
    "\n",
    "Execution order matters; run cells sequentially.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b5df7a",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.11' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/workspaces/semantic-kernel/.python/bin/python -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# 1. Load .gitignore File\n",
    "from pathlib import Path\n",
    "import json, re, os, itertools\n",
    "\n",
    "GITIGNORE_PATH = Path(\".gitignore\")\n",
    "raw_lines = GITIGNORE_PATH.read_text(encoding=\"utf-8\").splitlines()\n",
    "print(f\"Loaded {len(raw_lines)} lines from {GITIGNORE_PATH}\")\n",
    "raw_preview = \"\\n\".join(raw_lines[:20])\n",
    "print(raw_preview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b767975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Segment Sections By Comment Headers\n",
    "from collections import defaultdict\n",
    "\n",
    "section_map = defaultdict(list)\n",
    "current_section = \"UNLABELED\"\n",
    "header_pattern = re.compile(r\"^#\\s*-{2,}\\s*$\")\n",
    "\n",
    "for line in raw_lines:\n",
    "    if line.startswith(\"#\"):\n",
    "        # treat non-empty comment lines as potential headers\n",
    "        if header_pattern.match(line):\n",
    "            continue\n",
    "        header_text = line.lstrip(\"#\").strip()\n",
    "        if header_text:\n",
    "            current_section = header_text\n",
    "            section_map[current_section]  # ensure key exists\n",
    "            continue\n",
    "    section_map[current_section].append(line)\n",
    "\n",
    "print(f\"Detected {len(section_map)} sections\")\n",
    "print(list(section_map.keys())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ad2334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Normalize & Canonicalize Patterns\n",
    "\n",
    "\n",
    "def normalize_pattern(p: str) -> str:\n",
    "    p = p.strip()\n",
    "    if not p or p.startswith(\"#\"):\n",
    "        return p\n",
    "    p = p.replace(\"\\\\\", \"/\")\n",
    "    p = re.sub(r\"/+\", \"/\", p)\n",
    "    return p\n",
    "\n",
    "\n",
    "normalized = []\n",
    "for idx, line in enumerate(raw_lines):\n",
    "    n = normalize_pattern(line)\n",
    "    normalized.append(\n",
    "        {\"index\": idx, \"original\": line, \"normalized\": n, \"changed\": line != n}\n",
    "    )\n",
    "\n",
    "print(\"Sample normalized entries:\")\n",
    "for row in normalized[:15]:\n",
    "    if row[\"changed\"]:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883b0244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Classify Patterns (Language / Purpose)\n",
    "\n",
    "TAGS = {\n",
    "    \"python\": [\n",
    "        \".pyc\",\n",
    "        \"__pycache__\",\n",
    "        \".pytest_cache\",\n",
    "        \"venv\",\n",
    "        \".venv\",\n",
    "        \"pdm\",\n",
    "        \"poetry.lock\",\n",
    "        \"pipfile\",\n",
    "    ],\n",
    "    \"dotnet\": [\".sln\", \"bin/\", \"obj/\", \".csproj\", \".vs/\"],\n",
    "    \"node\": [\"node_modules\", \"package-lock.json\", \"playwright\"],\n",
    "    \"logs\": [\".log\", \"logs/\", \"log/\"],\n",
    "    \"env\": [\".env\", \"env/\"],\n",
    "    \"build\": [\"dist/\", \"build/\", \"coverage\", \"BenchmarkDotNet.Artifacts\"],\n",
    "    \"cert\": [\".pem\", \".crt\", \".key\", \".pfx\"],\n",
    "}\n",
    "\n",
    "\n",
    "def classify(p: str) -> List[str]:\n",
    "    if not p or p.startswith(\"#\"):\n",
    "        return []\n",
    "    tags = []\n",
    "    for tag, needles in TAGS.items():\n",
    "        if any(n in p for n in needles):\n",
    "            tags.append(tag)\n",
    "    return tags\n",
    "\n",
    "\n",
    "for row in normalized:\n",
    "    row[\"tags\"] = classify(row[\"normalized\"])\n",
    "\n",
    "print(\"Tagged counts:\")\n",
    "from collections import Counter\n",
    "\n",
    "c = Counter(itertools.chain.from_iterable(r[\"tags\"] for r in normalized))\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14462f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Detect Duplicates & Shadowed Patterns\n",
    "\n",
    "seen = {}\n",
    "exact_duplicates = []\n",
    "# gitignore precedence: later rules override earlier; we detect earlier broad rules overshadowing specifics\n",
    "shadowed = []\n",
    "\n",
    "for idx, row in enumerate(normalized):\n",
    "    pat = row[\"normalized\"]\n",
    "    if not pat or pat.startswith(\"#\"):\n",
    "        continue\n",
    "    if pat in seen:\n",
    "        exact_duplicates.append(\n",
    "            {\"pattern\": pat, \"first_index\": seen[pat], \"dup_index\": idx}\n",
    "        )\n",
    "    else:\n",
    "        seen[pat] = idx\n",
    "\n",
    "# Simple heuristic: if a pattern is a prefix directory of another later pattern\n",
    "patterns = [\n",
    "    r[\"normalized\"]\n",
    "    for r in normalized\n",
    "    if r[\"normalized\"] and not r[\"normalized\"].startswith(\"#\")\n",
    "]\n",
    "for i, a in enumerate(patterns):\n",
    "    if a.endswith(\"/\"):\n",
    "        for j, b in enumerate(patterns):\n",
    "            if j <= i:\n",
    "                continue\n",
    "            if b.startswith(a) and b != a:\n",
    "                shadowed.append({\"broad\": a, \"specific\": b})\n",
    "\n",
    "print(\n",
    "    f\"Exact duplicates: {len(exact_duplicates)} | Potential shadowed: {len(shadowed)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bef77fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Glob Expansion: Enumerate Currently Ignored Files\n",
    "import subprocess, shlex\n",
    "\n",
    "# Collect candidate file paths (limit for performance)\n",
    "all_paths = []\n",
    "for root, dirs, files in os.walk(\".\", topdown=True):\n",
    "    # skip .git itself\n",
    "    if root.startswith(\"./.git\"):\n",
    "        continue\n",
    "    for f in files:\n",
    "        p = os.path.join(root, f)\n",
    "        all_paths.append(p)\n",
    "    if len(all_paths) > 5000:\n",
    "        break\n",
    "\n",
    "# Use git check-ignore batch\n",
    "proc = subprocess.run(\n",
    "    [\"git\", \"check-ignore\", \"--stdin\"],\n",
    "    input=\"\\n\".join(all_paths),\n",
    "    text=True,\n",
    "    capture_output=True,\n",
    ")\n",
    "ignored_set = set(proc.stdout.splitlines()) if proc.returncode in (0, 1) else set()\n",
    "print(f\"Scanned {len(all_paths)} files, ignored {len(ignored_set)}\")\n",
    "\n",
    "ignored_samples = list(itertools.islice((p for p in all_paths if p in ignored_set), 20))\n",
    "print(\"Ignored sample:\")\n",
    "for s in ignored_samples:\n",
    "    print(\"  \", s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed44e581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Potential Redundant Artifact Patterns\n",
    "redundant = []\n",
    "# Heuristic: if pattern A directory covers pattern B file and both appear\n",
    "for row in normalized:\n",
    "    p = row[\"normalized\"]\n",
    "    if not p or p.startswith(\"#\") or p == \"/\":\n",
    "        continue\n",
    "    if p.endswith(\"/\"):\n",
    "        for other in normalized:\n",
    "            q = other[\"normalized\"]\n",
    "            if q and not q.startswith(\"#\") and q.startswith(p) and q != p:\n",
    "                redundant.append({\"dir\": p, \"child\": q})\n",
    "\n",
    "print(f\"Potential redundant pairs: {len(redundant)} (first 15 shown)\")\n",
    "for pair in redundant[:15]:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae9346b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Check Unignored (Negated) Patterns Integrity\n",
    "negations = [r[\"normalized\"] for r in normalized if r[\"normalized\"].startswith(\"!\")]\n",
    "ineffective = []\n",
    "\n",
    "# For each negation, test if any file currently ignored would be re-included\n",
    "for neg in negations:\n",
    "    pat = neg[1:]\n",
    "    # naive match: substring for quick heuristic\n",
    "    matches = [p for p in ignored_set if pat.strip(\"/\") in p]\n",
    "    if not matches:\n",
    "        ineffective.append(neg)\n",
    "\n",
    "print(f\"Negations found: {len(negations)} | Ineffective: {len(ineffective)}\")\n",
    "print(ineffective[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88b1aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Scan Repo For Large Tracked Artifacts Not Ignored\n",
    "LARGE_THRESHOLD = 5 * 1024 * 1024  # 5MB\n",
    "large_files = []\n",
    "for root, dirs, files in os.walk(\".\", topdown=True):\n",
    "    if root.startswith(\"./.git\"):\n",
    "        continue\n",
    "    for f in files:\n",
    "        path = os.path.join(root, f)\n",
    "        try:\n",
    "            sz = os.path.getsize(path)\n",
    "        except OSError:\n",
    "            continue\n",
    "        if sz >= LARGE_THRESHOLD and path not in ignored_set:\n",
    "            large_files.append({\"path\": path, \"size_mb\": round(sz / 1024 / 1024, 2)})\n",
    "\n",
    "print(f\"Large tracked files (>=5MB) not ignored: {len(large_files)} (first 10)\")\n",
    "for lf in large_files[:10]:\n",
    "    print(lf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5829b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Simulate Ignore Matching For Sample Paths\n",
    "import fnmatch\n",
    "\n",
    "rules = [\n",
    "    r[\"normalized\"]\n",
    "    for r in normalized\n",
    "    if r[\"normalized\"] and not r[\"normalized\"].startswith(\"#\")\n",
    "]\n",
    "\n",
    "\n",
    "def match_path(path: str):\n",
    "    # Follows order: later rules override earlier\n",
    "    matched_rule = None\n",
    "    is_ignored = False\n",
    "    for rule in rules:\n",
    "        neg = rule.startswith(\"!\")\n",
    "        pattern = rule[1:] if neg else rule\n",
    "        # crude glob match\n",
    "        if fnmatch.fnmatch(path, pattern) or path.startswith(pattern.rstrip(\"/\")):\n",
    "            if neg:\n",
    "                is_ignored = False\n",
    "                matched_rule = rule\n",
    "            else:\n",
    "                is_ignored = True\n",
    "                matched_rule = rule\n",
    "    return {\"path\": path, \"ignored\": is_ignored, \"rule\": matched_rule}\n",
    "\n",
    "\n",
    "# Demo\n",
    "for demo in [\"dist/output.bin\", \"node_modules/pkg/index.js\", \"docs/README.md\"]:\n",
    "    print(match_path(demo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee25b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Suggest Consolidated Pattern Groups\n",
    "\n",
    "suggestions = []\n",
    "# Group by tag sets for potential consolidation\n",
    "from collections import defaultdict\n",
    "\n",
    "by_tagset = defaultdict(list)\n",
    "for r in normalized:\n",
    "    pats = tuple(sorted(r.get(\"tags\", [])))\n",
    "    if pats:\n",
    "        by_tagset[pats].append(r[\"normalized\"])\n",
    "\n",
    "for tags, pats in by_tagset.items():\n",
    "    if len(pats) > 3:\n",
    "        suggestions.append({\"tags\": tags, \"count\": len(pats), \"sample\": pats[:5]})\n",
    "\n",
    "print(f\"Consolidation candidate groups: {len(suggestions)}\")\n",
    "for s in suggestions[:10]:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4662b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Generate Cleaned .gitignore Draft\n",
    "\n",
    "\n",
    "def build_draft():\n",
    "    lines = []\n",
    "    lines.append(\"# Auto-generated draft (DO NOT COMMIT without review)\")\n",
    "    grouped = defaultdict(list)\n",
    "    for r in normalized:\n",
    "        pat = r[\"normalized\"]\n",
    "        if not pat or pat.startswith(\"#\"):\n",
    "            continue\n",
    "        key = tuple(sorted(r.get(\"tags\", []))) or (\"misc\",)\n",
    "        grouped[key].append(pat)\n",
    "    for key, pats in sorted(grouped.items(), key=lambda x: (-len(x[1]), x[0])):\n",
    "        lines.append(\"\")\n",
    "        lines.append(\n",
    "            f\"# Group: {', '.join([k for k in key if k])} ({len(pats)} patterns)\"\n",
    "        )\n",
    "        seen_local = set()\n",
    "        for p in pats:\n",
    "            if p in seen_local:\n",
    "                continue\n",
    "            seen_local.add(p)\n",
    "            lines.append(p)\n",
    "    return \"\\n\".join(lines) + \"\\n\"\n",
    "\n",
    "\n",
    "draft = build_draft()\n",
    "print(\"\\n\".join(draft.splitlines()[:40]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adec3d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Validate Draft Against Current Git Status (Dry Run)\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "validation_report = {}\n",
    "with TemporaryDirectory() as tmp:\n",
    "    draft_path = Path(tmp) / \".gitignore\"\n",
    "    draft_path.write_text(draft, encoding=\"utf-8\")\n",
    "    # Copy only .git directory reference by using git check-ignore with --no-index referencing draft\n",
    "    sample_check = subprocess.run(\n",
    "        [\"git\", \"check-ignore\", \"-n\", \"--stdin\"],\n",
    "        input=\"\\n\".join(all_paths[:200]),\n",
    "        text=True,\n",
    "        capture_output=True,\n",
    "    )\n",
    "    validation_report[\"sample_output\"] = sample_check.stdout.splitlines()[:40]\n",
    "\n",
    "print(\"Validation sample lines:\")\n",
    "for line in validation_report[\"sample_output\"]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff877a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. Export Reports (JSON / Markdown)\n",
    "import json\n",
    "\n",
    "REPORT_DIR = Path(\"gitignore_audit_reports\")\n",
    "REPORT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "(REPORT_DIR / \"duplicates.json\").write_text(json.dumps(exact_duplicates, indent=2))\n",
    "(REPORT_DIR / \"shadowed.json\").write_text(json.dumps(shadowed, indent=2))\n",
    "(REPORT_DIR / \"ineffective_negations.json\").write_text(\n",
    "    json.dumps(ineffective, indent=2)\n",
    ")\n",
    "(REPORT_DIR / \"large_unignored.json\").write_text(json.dumps(large_files, indent=2))\n",
    "(REPORT_DIR / \"redundant_pairs.json\").write_text(json.dumps(redundant[:200], indent=2))\n",
    "(REPORT_DIR / \"proposed.gitignore\").write_text(draft)\n",
    "\n",
    "summary_md = f\"\"\"# .gitignore Audit Summary\\n\\n* Total lines: {len(raw_lines)}\\n* Exact duplicates: {len(exact_duplicates)}\\n* Potential shadowed: {len(shadowed)}\\n* Redundant pairs (dir->child): {len(redundant)}\\n* Ineffective negations: {len(ineffective)}\\n* Large tracked (>=5MB) not ignored: {len(large_files)}\\n\\nSee JSON artifacts for details.\\n\"\"\"\n",
    "(REPORT_DIR / \"summary.md\").write_text(summary_md)\n",
    "\n",
    "print(\"Artifacts written to\", REPORT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c12a90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Send a chat completion request to local AI server\n",
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"http://192.168.0.154:1234/v1/chat/completions\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "payload = {\n",
    "    \"model\": \"openai/gpt-oss-20b\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Hello, local model!\"}],\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
    "if response.ok:\n",
    "    result = response.json()\n",
    "    print(result[\"choices\"][0][\"message\"][\"content\"])\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\", response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".python (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
