{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "614cf014",
   "metadata": {},
   "source": [
    "# 1. Choose Topic And Objectives\n",
    "\n",
    "Define the analytical focus, success metrics, and guiding questions.\n",
    "\n",
    "**Proposed Topic:** Adaptive AGI MCP Capability & Data Pipeline Scaffold\n",
    "\n",
    "**Initial Objectives:**\n",
    "\n",
    "- Enumerate AGI MCP server tool surface (capability inventory)\n",
    "- Inspect local AGI memory (SQLite) structure & counts\n",
    "- Provide reusable data processing scaffold (extensible to future telemetry)\n",
    "- Establish validation & profiling hooks\n",
    "\n",
    "**Success Metrics (initial):**\n",
    "\n",
    "- Notebook runs top-to-bottom without modification\n",
    "- All helper functions unit-tested in-line\n",
    "- Summary cell produces consolidated capability report dict\n",
    "\n",
    "**Key Questions:**\n",
    "\n",
    "1. What kernel_function tools are currently exposed?\n",
    "2. What data structures exist in `agi_memory.db`?\n",
    "3. Which transformation steps are reusable across future AGI telemetry sources?\n",
    "4. How can we parameterize runs (paths, sample sizes) for CI or papermill automation?\n",
    "\n",
    "---\n",
    "\n",
    "Parameters cell follows (editable).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c014210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters (modifiable for papermill)\n",
    "PROJECT_NAME = 'agi_mcp_scaffold'\n",
    "OBJECTIVES = [\n",
    "    'inventory_capabilities',\n",
    "    'introspect_memory',\n",
    "    'establish_processing_pipeline',\n",
    "]\n",
    "CONFIG = {}\n",
    "PARAMS = {\n",
    "    'INPUT_PATH': 'data/raw',\n",
    "    'PROCESSED_PATH': 'data/processed',\n",
    "    'RAW_DB_PATH': 'agi_memory.db',\n",
    "    'SAMPLE_SIZE': 1000,\n",
    "    'ENABLE_PROFILING': True,\n",
    "}\n",
    "print('Parameters loaded:', PARAMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20df6a69",
   "metadata": {},
   "source": [
    "# 2. Set Up Environment\n",
    "\n",
    "Prepare imports, version reporting, and global options.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3020f8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports & environment configuration\n",
    "from __future__ import annotations\n",
    "import sys, os, json, math, logging, sqlite3, inspect, importlib\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Iterable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.width', 120)\n",
    "\n",
    "ROOT = Path.cwd()\n",
    "DATA_RAW = ROOT / 'data' / 'raw'\n",
    "DATA_PROCESSED = ROOT / 'data' / 'processed'\n",
    "FIG_DIR = ROOT / 'reports' / 'figures'\n",
    "for d in (DATA_RAW, DATA_PROCESSED, FIG_DIR):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('Python', sys.version)\n",
    "print('Pandas', pd.__version__, 'NumPy', np.__version__)\n",
    "print('Working directory:', ROOT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fddea66",
   "metadata": {},
   "source": [
    "# 3. Data Acquisition Or Generation\n",
    "\n",
    "Scaffold for loading raw data or generating synthetic samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2a68bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data acquisition & synthetic generation\n",
    "from datetime import datetime\n",
    "\n",
    "def load_raw(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load a CSV (or JSON) file into a DataFrame; returns empty df if not exists.\"\"\"\n",
    "    if not path.exists():\n",
    "        logging.warning('Raw path %s does not exist; returning empty DataFrame', path)\n",
    "        return pd.DataFrame()\n",
    "    if path.suffix.lower() == '.csv':\n",
    "        return pd.read_csv(path)\n",
    "    if path.suffix.lower() == '.json':\n",
    "        return pd.read_json(path)\n",
    "    raise ValueError(f'Unsupported extension: {path.suffix}')\n",
    "\n",
    "\n",
    "def generate_synthetic(n: int = 200) -> pd.DataFrame:\n",
    "    \"\"\"Generate deterministic synthetic telemetry-like data set.\"\"\"\n",
    "    rng = np.random.default_rng(42)\n",
    "    ts = pd.date_range(datetime(2024, 1, 1), periods=n, freq='H')\n",
    "    df = pd.DataFrame({\n",
    "        'timestamp': ts,\n",
    "        'session_id': rng.integers(1000, 9999, size=n),\n",
    "        'latency_ms': rng.normal(250, 40, size=n).round(1),\n",
    "        'tokens': rng.poisson(120, size=n),\n",
    "        'tool_invocations': rng.poisson(3, size=n),\n",
    "        'error_flag': rng.choice([0,1], size=n, p=[0.93, 0.07])\n",
    "    })\n",
    "    return df\n",
    "\n",
    "RAW_CACHE = DATA_RAW / 'synthetic_raw.csv'\n",
    "if RAW_CACHE.exists():\n",
    "    df_raw = pd.read_csv(RAW_CACHE, parse_dates=['timestamp'])\n",
    "else:\n",
    "    df_raw = generate_synthetic(300)\n",
    "    df_raw.to_csv(RAW_CACHE, index=False)\n",
    "\n",
    "print('Raw rows:', len(df_raw), 'cached at', RAW_CACHE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e067784",
   "metadata": {},
   "source": [
    "# 4. Exploratory Data Inspection\n",
    "\n",
    "Inspect structure, schema, and simple distributions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8881176b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic inspection & summary helper\n",
    "from collections import OrderedDict\n",
    "\n",
    "def summarize(df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    if df.empty:\n",
    "        return {'empty': True}\n",
    "    miss = (df.isna().sum() / len(df) * 100).round(2).to_dict()\n",
    "    return {\n",
    "        'shape': df.shape,\n",
    "        'columns': list(df.columns),\n",
    "        'dtypes': df.dtypes.astype(str).to_dict(),\n",
    "        'missing_pct': miss,\n",
    "    }\n",
    "\n",
    "# Perform inspection\n",
    "print(df_raw.head(3))\n",
    "print('\\nInfo:')\n",
    "print(df_raw.info())\n",
    "print('\\nDescribe:')\n",
    "print(df_raw.describe(include='all'))\n",
    "print('\\nSummary dict:')\n",
    "print(json.dumps(summarize(df_raw), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e53a212",
   "metadata": {},
   "source": [
    "# 5. Core Implementation\n",
    "\n",
    "Define transformation pipeline entrypoints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c4e503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core transformation logic\n",
    "\n",
    "def process(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Primary deterministic transformation pipeline.\n",
    "\n",
    "    Steps:\n",
    "      - Ensure timestamp sorted\n",
    "      - Add rolling latency mean (window=6)\n",
    "      - Compute tokens_per_invocation (safe divide)\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return df.copy()\n",
    "    out = df.sort_values('timestamp').reset_index(drop=True).copy()\n",
    "    out['latency_ms_rolling6'] = out['latency_ms'].rolling(6, min_periods=1).mean()\n",
    "    out['tokens_per_invocation'] = out.apply(lambda r: r['tokens'] / r['tool_invocations'] if r['tool_invocations'] else np.nan, axis=1)\n",
    "    return out\n",
    "\n",
    "# Mini example test\n",
    "_example = pd.DataFrame({\n",
    "    'timestamp': pd.date_range('2024-01-01', periods=3, freq='H'),\n",
    "    'latency_ms': [100, 200, 300],\n",
    "    'tokens': [10, 20, 30],\n",
    "    'tool_invocations': [1, 2, 0],\n",
    "    'session_id': [111,112,113],\n",
    "    'error_flag': [0,0,1]\n",
    "})\n",
    "_processed = process(_example)\n",
    "assert 'latency_ms_rolling6' in _processed.columns\n",
    "assert math.isclose(_processed.loc[1, 'latency_ms_rolling6'], 150)\n",
    "assert math.isnan(_processed.loc[2, 'tokens_per_invocation'])  # divide by zero case\n",
    "print('process() example tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdfc71b",
   "metadata": {},
   "source": [
    "# 6. Visualization\n",
    "\n",
    "Reusable plotting utilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd500af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting helpers\n",
    "from itertools import islice\n",
    "\n",
    "def plot_distributions(df: pd.DataFrame, cols: Iterable[str], prefix: str = 'dist') -> None:\n",
    "    cols = [c for c in cols if c in df.columns]\n",
    "    for c in cols:\n",
    "        fig, ax = plt.subplots(1,2, figsize=(8,3))\n",
    "        df[c].hist(ax=ax[0], bins=30)\n",
    "        ax[0].set_title(f'Histogram {c}')\n",
    "        df.boxplot(column=c, ax=ax[1])\n",
    "        ax[1].set_title(f'Box {c}')\n",
    "        fig.tight_layout()\n",
    "        outp = FIG_DIR / f'{prefix}_{c}.png'\n",
    "        fig.savefig(outp)\n",
    "        plt.close(fig)\n",
    "\n",
    "# Quick demo (subset to limit output)\n",
    "plot_distributions(df_raw, ['latency_ms','tokens','tool_invocations'])\n",
    "print('Saved distribution figs to', FIG_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90922d7c",
   "metadata": {},
   "source": [
    "# 7. Testing And Validation\n",
    "\n",
    "Lightweight invariants & validation helpers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524d1ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation helpers\n",
    "\n",
    "def validate(df: pd.DataFrame) -> List[str]:\n",
    "    issues: List[str] = []\n",
    "    if df.empty:\n",
    "        issues.append('DataFrame empty')\n",
    "        return issues\n",
    "    required = {'timestamp','latency_ms','tokens'}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        issues.append(f'Missing columns: {missing}')\n",
    "    if (df['latency_ms'] < 0).any():\n",
    "        issues.append('Negative latency_ms detected')\n",
    "    return issues\n",
    "\n",
    "processed = process(df_raw)\n",
    "problems = validate(processed)\n",
    "assert not problems, f'Validation issues: {problems}'\n",
    "print('Validation passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef245e8",
   "metadata": {},
   "source": [
    "# 8. Performance Profiling\n",
    "\n",
    "Time-critical routines & profiling scaffolds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277ecaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profiling examples (lightweight)\n",
    "if PARAMS.get('ENABLE_PROFILING'):\n",
    "    import cProfile, pstats, io\n",
    "    pr = cProfile.Profile()\n",
    "    pr.enable()\n",
    "    _ = process(df_raw)\n",
    "    pr.disable()\n",
    "    s = io.StringIO()\n",
    "    ps = pstats.Stats(pr, stream=s).sort_stats('tottime')\n",
    "    ps.print_stats(10)\n",
    "    print('cProfile top 10 by tottime:')\n",
    "    print(s.getvalue())\n",
    "else:\n",
    "    print('Profiling disabled by PARAMS.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab99ac3",
   "metadata": {},
   "source": [
    "# 9. Packaging Reusable Functions\n",
    "\n",
    "Refactor helpers into a module structure (conceptual demonstration).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4690c039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustrative export pattern (not writing file here to avoid side-effects)\n",
    "MODULE_SNIPPET = \"\"\"# src/utils.py\\nimport pandas as pd\\n\\ndef summarize(df: pd.DataFrame) -> dict:\\n    return {'shape': df.shape, 'columns': list(df.columns)}\\n\"\"\"\n",
    "print(MODULE_SNIPPET)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Example script usage\n",
    "    _ = process(df_raw)\n",
    "    print('Executed process() in __main__ context.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192107f2",
   "metadata": {},
   "source": [
    "# 10. Parameterization With Papermill\n",
    "\n",
    "Enable batch runs with overridden parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacbf619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Papermill parameters cell (tag manually if needed in UI)\n",
    "# Example (commented) CLI:\n",
    "# papermill AGI_Analysis_Notebook.ipynb output.ipynb -p SAMPLE_SIZE 500 -p ENABLE_PROFILING False\n",
    "PARAMS['SAMPLE_SIZE'] = int(PARAMS.get('SAMPLE_SIZE', 1000))\n",
    "print('Papermill-ready PARAMS:', PARAMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc910bc",
   "metadata": {},
   "source": [
    "# 11. Command Line Integration Via argparse\n",
    "\n",
    "Prototype CLI wrapping process() pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efed06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLI prototype (not writing to file for safety)\n",
    "CLI_SCRIPT = \"\"\"#!/usr/bin/env python3\\nimport argparse, pandas as pd\\nfrom pathlib import Path\\nfrom your_module import process  # replace with actual import path\\n\\nparser = argparse.ArgumentParser()\\nparser.add_argument('--input', required=True)\\nparser.add_argument('--out', required=True)\\nargs = parser.parse_args()\\n\\ndf = pd.read_csv(args.input)\\nout = process(df)\\nout.to_csv(args.out, index=False)\\nprint('Wrote', args.out)\\n\"\"\"\n",
    "print(CLI_SCRIPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b183de3",
   "metadata": {},
   "source": [
    "# 12. Unit Tests In VS Code Test Explorer\n",
    "\n",
    "Inline pytest-style example snippet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f353a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytest snippet (not executed here)\n",
    "TEST_SNIPPET = \"\"\"# tests/test_process.py\\nimport pandas as pd\\nfrom your_module import process\\n\\ndef test_process_columns():\\n    df = pd.DataFrame({'timestamp':[0], 'latency_ms':[1], 'tokens':[2], 'tool_invocations':[1]})\\n    out = process(df)\\n    assert 'tokens_per_invocation' in out.columns\\n\"\"\"\n",
    "print(TEST_SNIPPET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293f19e5",
   "metadata": {},
   "source": [
    "# 13. Logging And Debug Output\n",
    "\n",
    "Standard logging configuration & usage demo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0040b236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging configuration\n",
    "time_format = '%Y-%m-%d %H:%M:%S'\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s', datefmt=time_format)\n",
    "logger = logging.getLogger('agi_notebook')\n",
    "logger.info('Logger initialized.')\n",
    "\n",
    "logger.debug('This is a DEBUG message (may be hidden).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b15042",
   "metadata": {},
   "source": [
    "# 14. Saving Artifacts\n",
    "\n",
    "Persist processed data and metadata for downstream steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67575f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data & metadata\n",
    "processed_path = DATA_PROCESSED / 'processed.parquet'\n",
    "processed.to_parquet(processed_path, index=False)\n",
    "meta = {\n",
    "    'rows': len(processed),\n",
    "    'generated_at': datetime.utcnow().isoformat() + 'Z',\n",
    "    'project': PROJECT_NAME,\n",
    "}\n",
    "meta_path = DATA_PROCESSED / 'metadata.json'\n",
    "with meta_path.open('w') as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "print('Saved processed data to', processed_path)\n",
    "print('Saved metadata to', meta_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebf762c",
   "metadata": {},
   "source": [
    "# 15. Next Steps / TODO\n",
    "\n",
    "- [ ] Integrate real AGI MCP capability introspection\n",
    "- [ ] Extend validation with statistical anomaly detection\n",
    "- [ ] Add streaming telemetry ingestion example\n",
    "- [ ] Wire notebook into CI via papermill\n",
    "- [ ] Implement advanced profiling (line_profiler)\n",
    "- [ ] Add schema versioning for processed outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227f8737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder cell for future experimental code\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c2eaf1",
   "metadata": {},
   "source": [
    "# 16. AGI MCP Capability Introspection\n",
    "\n",
    "Inventory kernel_function decorated tools from mcp-agi-server (best-effort).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c013bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt dynamic import of AGI MCP server and extract kernel_function tools\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "\n",
    "AGI_SERVER_PATH = Path('02-ai-workspace') / 'mcp-agi-server.py'\n",
    "capability_report = {\n",
    "    'import_success': False,\n",
    "    'capabilities': [],\n",
    "    'fallback_regex_used': False,\n",
    "    'path_exists': AGI_SERVER_PATH.exists(),\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Dynamic module load\n",
    "    spec = importlib.util.spec_from_file_location('mcp_agi_server_dyn', AGI_SERVER_PATH)\n",
    "    if spec and spec.loader:\n",
    "        mod = importlib.util.module_from_spec(spec)\n",
    "        spec.loader.exec_module(mod)  # type: ignore\n",
    "        capability_report['import_success'] = True\n",
    "        # Look for callables with attribute 'kernel_function' or decorated names\n",
    "        for name, obj in inspect.getmembers(mod):\n",
    "            if inspect.isfunction(obj):\n",
    "                if getattr(obj, '__name__', '').startswith('_'):\n",
    "                    continue\n",
    "                # Heuristic: presence of parameter doc or typical tool names\n",
    "                src = inspect.getsource(obj)\n",
    "                if 'kernel_function' in src or 'kernel_function(' in src:\n",
    "                    capability_report['capabilities'].append({\n",
    "                        'name': name,\n",
    "                        'signature': str(inspect.signature(obj)),\n",
    "                        'doc': (inspect.getdoc(obj) or '')[:300]\n",
    "                    })\n",
    "        # Also scan classes for methods\n",
    "        for cname, cobj in inspect.getmembers(mod, inspect.isclass):\n",
    "            for mname, mobj in inspect.getmembers(cobj, inspect.isfunction):\n",
    "                try:\n",
    "                    src = inspect.getsource(mobj)\n",
    "                except OSError:\n",
    "                    continue\n",
    "                if 'kernel_function' in src:\n",
    "                    capability_report['capabilities'].append({\n",
    "                        'name': f'{cname}.{mname}',\n",
    "                        'signature': str(inspect.signature(mobj)),\n",
    "                        'doc': (inspect.getdoc(mobj) or '')[:300]\n",
    "                    })\n",
    "except Exception as e:  # noqa: BLE001\n",
    "    capability_report['error'] = repr(e)\n",
    "\n",
    "# Fallback: regex parse file for def lines following @kernel_function\n",
    "if (not capability_report['import_success']) and AGI_SERVER_PATH.exists():\n",
    "    text = AGI_SERVER_PATH.read_text(encoding='utf-8', errors='ignore')\n",
    "    pattern = r'@.*kernel_function.*\\ndef\\s+(\\w+)\\s*\\(([^\\)]*)\\)'  # simple heuristic\n",
    "    matches = re.findall(pattern, text)\n",
    "    capability_report['fallback_regex_used'] = True\n",
    "    for func_name, params in matches:\n",
    "        capability_report['capabilities'].append({\n",
    "            'name': func_name,\n",
    "            'signature': f'({params})',\n",
    "            'doc': ''\n",
    "        })\n",
    "\n",
    "print('AGI capability report:')\n",
    "print(json.dumps(capability_report, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86cef12",
   "metadata": {},
   "source": [
    "## 7a. Statistical Anomaly Detection\n",
    "\n",
    "Z-score based detection for latency_ms and tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93272b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-score anomaly detection (simple)\n",
    "from typing import Tuple\n",
    "\n",
    "def detect_anomalies(df: pd.DataFrame, cols: List[str], z_thresh: float = 3.0) -> Dict[str, Any]:\n",
    "    results: Dict[str, Any] = {'threshold': z_thresh, 'columns': {}, 'total_flags': 0}\n",
    "    if df.empty:\n",
    "        return results\n",
    "    for c in cols:\n",
    "        if c not in df.columns:\n",
    "            continue\n",
    "        series = df[c].astype(float)\n",
    "        mu, sigma = series.mean(), series.std(ddof=0)\n",
    "        if sigma == 0:\n",
    "            flags = []\n",
    "        else:\n",
    "            z = (series - mu).abs() / sigma\n",
    "            flags = series.index[z > z_thresh].tolist()\n",
    "        results['columns'][c] = {\n",
    "            'mean': mu,\n",
    "            'std': sigma,\n",
    "            'anomaly_indices': flags,\n",
    "            'anomaly_count': len(flags)\n",
    "        }\n",
    "        results['total_flags'] += len(flags)\n",
    "    return results\n",
    "\n",
    "anomaly_results = detect_anomalies(processed, ['latency_ms','tokens'])\n",
    "print(json.dumps(anomaly_results, indent=2)[:2000])  # truncate for display\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
