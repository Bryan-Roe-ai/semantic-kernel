#!/bin/bash
# Local AI Quick Launch Script
#
# Copyright (c) 2025 Bryan Roe
# Licensed under the MIT License
#
# This file is part of the Semantic Kernel - Advanced AI Development Framework.
# Original work by Bryan Roe.
#
# Author: Bryan Roe
# Created: 2025
# License: MIT

set -e

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
CYAN='\033[0;36m'
BOLD='\033[1m'
NC='\033[0m' # No Color

# Configuration
WORKSPACE_ROOT="/workspaces/semantic-kernel"
BACKEND_DIR="$WORKSPACE_ROOT/19-miscellaneous/src"
FRONTEND_DIR="$WORKSPACE_ROOT/07-resources/public"
LM_STUDIO_URL="http://localhost:1234"
BACKEND_URL="http://localhost:8000"

print_header() {
    echo -e "${CYAN}â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
    echo -e "â•‘                           ðŸš€ LOCAL AI QUICK LAUNCH ðŸš€                        â•‘"
    echo -e "â•‘                        Semantic Kernel Local AI System                      â•‘"
    echo -e "â•‘                             by Bryan Roe (2025)                            â•‘"
    echo -e "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
    echo ""
}

check_dependency() {
    if command -v "$1" &> /dev/null; then
        echo -e "  ${GREEN}âœ“${NC} $1"
        return 0
    else
        echo -e "  ${RED}âœ—${NC} $1 (not found)"
        return 1
    fi
}

check_python_package() {
    if python3 -c "import $1" &> /dev/null; then
        echo -e "  ${GREEN}âœ“${NC} $1"
        return 0
    else
        echo -e "  ${RED}âœ—${NC} $1 (not installed)"
        return 1
    fi
}

check_lm_studio() {
    echo -e "${BLUE}ðŸ” Checking LM Studio...${NC}"

    if curl -s "$LM_STUDIO_URL/v1/models" &> /dev/null; then
        models=$(curl -s "$LM_STUDIO_URL/v1/models" | python3 -c "import json,sys; print(len(json.load(sys.stdin).get('data', [])))" 2>/dev/null || echo "0")
        echo -e "  ${GREEN}âœ“${NC} LM Studio is running with $models models available"
        return 0
    else
        echo -e "  ${RED}âœ—${NC} LM Studio is not running or not accessible"
        echo -e "  ${YELLOW}ðŸ’¡ Please start LM Studio and enable the API server${NC}"
        return 1
    fi
}

check_ollama() {
    echo -e "${BLUE}ðŸ” Checking Ollama...${NC}"

    if curl -s "http://localhost:11434/api/tags" &> /dev/null; then
        models=$(curl -s "http://localhost:11434/api/tags" | python3 -c "import json,sys; print(len(json.load(sys.stdin).get('models', [])))" 2>/dev/null || echo "0")
        echo -e "  ${GREEN}âœ“${NC} Ollama is running with $models models available"
        return 0
    else
        echo -e "  ${YELLOW}â„¹${NC} Ollama is not running (optional)"
        return 1
    fi
}

install_dependencies() {
    echo -e "${BLUE}ðŸ“¦ Installing dependencies...${NC}"

    pip3 install fastapi uvicorn requests pydantic starlette aiohttp &> /dev/null

    if [ $? -eq 0 ]; then
        echo -e "  ${GREEN}âœ“${NC} Dependencies installed successfully"
        return 0
    else
        echo -e "  ${RED}âœ—${NC} Failed to install dependencies"
        return 1
    fi
}

setup_environment() {
    echo -e "${BLUE}âš™ï¸ Setting up environment...${NC}"

    # Create directories
    mkdir -p "$BACKEND_DIR/plugins"
    mkdir -p "$BACKEND_DIR/uploads"

    # Create .env file
    cat > "$BACKEND_DIR/.env" << EOF
LM_STUDIO_URL="$LM_STUDIO_URL/v1/chat/completions"
EOF

    echo -e "  ${GREEN}âœ“${NC} Environment configured"
}

start_backend() {
    echo -e "${BLUE}ðŸš€ Starting backend server...${NC}"

    cd "$BACKEND_DIR"

    # Check if backend is already running
    if curl -s "$BACKEND_URL/ping" &> /dev/null; then
        echo -e "  ${YELLOW}â„¹${NC} Backend server is already running"
        return 0
    fi

    # Start backend in background
    nohup python3 -m uvicorn backend:app --reload --host 127.0.0.1 --port 8000 > backend.log 2>&1 &
    BACKEND_PID=$!

    # Wait for backend to start
    echo -e "  ${YELLOW}â³${NC} Waiting for backend to start..."
    sleep 5

    if curl -s "$BACKEND_URL/ping" &> /dev/null; then
        echo -e "  ${GREEN}âœ“${NC} Backend server started successfully (PID: $BACKEND_PID)"
        echo "$BACKEND_PID" > backend.pid
        return 0
    else
        echo -e "  ${RED}âœ—${NC} Backend server failed to start"
        return 1
    fi
}

open_interface() {
    echo -e "${BLUE}ðŸŒ Opening chat interface...${NC}"

    # Try to use BROWSER environment variable (for dev containers)
    if [ -n "$BROWSER" ]; then
        "$BROWSER" "file://$FRONTEND_DIR/ai-chat-launcher.html" &
    elif command -v xdg-open &> /dev/null; then
        xdg-open "file://$FRONTEND_DIR/ai-chat-launcher.html" &
    elif command -v open &> /dev/null; then
        open "file://$FRONTEND_DIR/ai-chat-launcher.html" &
    else
        echo -e "  ${YELLOW}ðŸ’¡${NC} Please open this URL in your browser:"
        echo -e "  file://$FRONTEND_DIR/ai-chat-launcher.html"
        return 1
    fi

    echo -e "  ${GREEN}âœ“${NC} Chat interface opened in browser"
    return 0
}

stop_backend() {
    echo -e "${BLUE}ðŸ›‘ Stopping backend server...${NC}"

    if [ -f "$BACKEND_DIR/backend.pid" ]; then
        PID=$(cat "$BACKEND_DIR/backend.pid")
        if kill -0 "$PID" 2>/dev/null; then
            kill "$PID"
            rm -f "$BACKEND_DIR/backend.pid"
            echo -e "  ${GREEN}âœ“${NC} Backend server stopped"
        else
            echo -e "  ${YELLOW}â„¹${NC} Backend server was not running"
            rm -f "$BACKEND_DIR/backend.pid"
        fi
    else
        # Try to find and kill uvicorn process
        pkill -f "uvicorn backend:app" 2>/dev/null || true
        echo -e "  ${GREEN}âœ“${NC} Backend processes stopped"
    fi
}

show_status() {
    echo -e "${BOLD}System Status:${NC}"
    echo ""

    echo -e "${BLUE}System Dependencies:${NC}"
    check_dependency "python3"
    check_dependency "pip3"
    check_dependency "curl"

    echo ""
    echo -e "${BLUE}Python Packages:${NC}"
    check_python_package "fastapi"
    check_python_package "uvicorn"
    check_python_package "requests"

    echo ""
    echo -e "${BLUE}AI Providers:${NC}"
    check_lm_studio
    check_ollama

    echo ""
    echo -e "${BLUE}Backend Service:${NC}"
    if curl -s "$BACKEND_URL/ping" &> /dev/null; then
        echo -e "  ${GREEN}âœ“${NC} Backend server is running"
    else
        echo -e "  ${RED}âœ—${NC} Backend server is not running"
    fi
}

show_help() {
    echo -e "${BOLD}Usage:${NC} $0 [COMMAND]"
    echo ""
    echo -e "${BOLD}Commands:${NC}"
    echo "  start     - Start the local AI system (default)"
    echo "  stop      - Stop the backend server"
    echo "  restart   - Restart the backend server"
    echo "  status    - Show system status"
    echo "  setup     - Setup dependencies and environment"
    echo "  help      - Show this help message"
    echo ""
    echo -e "${BOLD}Examples:${NC}"
    echo "  $0 start    # Start everything and open chat interface"
    echo "  $0 stop     # Stop the backend server"
    echo "  $0 status   # Check if everything is working"
}

main() {
    print_header

    case "${1:-start}" in
        "start")
            echo -e "${YELLOW}ðŸš€ Starting Local AI System...${NC}"
            echo ""

            # Check system
            show_status
            echo ""

            # Setup if needed
            if ! check_python_package "fastapi" &> /dev/null; then
                install_dependencies
                echo ""
            fi

            setup_environment
            echo ""

            # Start backend
            start_backend
            echo ""

            # Open interface
            open_interface
            echo ""

            echo -e "${GREEN}ðŸŽ‰ Local AI system is ready!${NC}"
            echo ""
            echo -e "${BOLD}Available interfaces:${NC}"
            echo "  â€¢ Advanced Chat: file://$FRONTEND_DIR/ai-chat-launcher.html"
            echo "  â€¢ Plugin Chat:   file://$FRONTEND_DIR/plugin-chat.html"
            echo "  â€¢ Simple Chat:   file://$FRONTEND_DIR/simple-chat.html"
            echo ""
            echo -e "${BOLD}API Documentation:${NC} $BACKEND_URL/docs"
            echo -e "${BOLD}Backend Logs:${NC} $BACKEND_DIR/backend.log"
            echo ""
            echo -e "${YELLOW}To stop the system, run:${NC} $0 stop"
            ;;

        "stop")
            stop_backend
            ;;

        "restart")
            stop_backend
            echo ""
            start_backend
            ;;

        "status")
            show_status
            ;;

        "setup")
            install_dependencies
            setup_environment
            echo -e "${GREEN}âœ“ Setup complete!${NC}"
            ;;

        "help"|"-h"|"--help")
            show_help
            ;;

        *)
            echo -e "${RED}Unknown command: $1${NC}"
            echo ""
            show_help
            exit 1
            ;;
    esac
}

# Handle Ctrl+C
trap 'echo -e "\n${YELLOW}Interrupted by user${NC}"; exit 130' INT

main "$@"
