{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "68e1c158",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
   "metadata": {},
   "source": [
    "# Using Hugging Face With Plugins\n",
    "\n",
    "In this notebook, we demonstrate using Hugging Face models for Plugins using both SemanticMemory and text completions. \n",
    "\n",
    "SK supports downloading models from the Hugging Face that can perform the following tasks: text-generation, text2text-generation, summarization, and sentence-similarity. You can search for models by task at https://huggingface.co/models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77bdf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install semantic-kernel==0.5.1.dev0\n",
    "\n",
    "# Note that additional dependencies are required for the Hugging Face connectors:\n",
    "!python -m pip install torch==2.0.0\n",
    "!python -m pip install transformers==^4.28.1\n",
    "!python -m pip install sentence-transformers==^2.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "508ad44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import semantic_kernel as sk\n",
    "import semantic_kernel.connectors.ai.hugging_face as sk_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "753ab756",
   "metadata": {},
   "outputs": [],
   "source": [
    "from services import Service\n",
    "\n",
    "# Select a service to use for this notebook (available services: OpenAI, AzureOpenAI, HuggingFace)\n",
    "selectedService = Service.HuggingFace"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8ddffc1",
   "metadata": {},
   "source": [
    "First, we will create a kernel and add both text completion and embedding services. \n",
    "\n",
    "For text completion, we are choosing GPT2. This is a text-generation model. (Note: text-generation will repeat the input in the output, text2text-generation will not.)\n",
    "For embeddings, we are using sentence-transformers/all-MiniLM-L6-v2. Vectors generated for this model are of length 384 (compared to a length of 1536 from OpenAI ADA).\n",
    "\n",
    "The following step may take a few minutes when run for the first time as the models will be downloaded to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f8dcbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = sk.Kernel()\n",
    "\n",
    "# Configure LLM service\n",
    "if selectedService == Service.HuggingFace:\n",
    "    text_service_id = \"gpt2\"\n",
    "    kernel.add_service(\n",
    "        service=sk_hf.HuggingFaceTextCompletion(\n",
    "            service_id=text_service_id, ai_model_id=text_service_id, task=\"text-generation\"\n",
    "        ),\n",
    "    )\n",
    "    embed_service_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    embedding_svc = sk_hf.HuggingFaceTextEmbedding(service_id=embed_service_id, ai_model_id=embed_service_id)\n",
    "    kernel.add_service(\n",
    "        service=embedding_svc,\n",
    "    )\n",
    "    kernel.use_memory(storage=sk.memory.VolatileMemoryStore(), embeddings_generator=embedding_svc)\n",
    "    kernel.import_plugin(sk.core_plugins.TextMemoryPlugin(), \"TextMemoryPlugin\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2a7e7ca4",
   "metadata": {},
   "source": [
    "### Add Memories and Define a plugin to use them\n",
    "\n",
    "Most models available on huggingface.co are not as powerful as OpenAI GPT-3+. Your plugins will likely need to be simpler to accommodate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d096504c",
   "metadata": {},
   "outputs": [],
   "source": [
    "await kernel.memory.save_information(collection=\"animal-facts\", id=\"info1\", text=\"Sharks are fish.\")\n",
    "await kernel.memory.save_information(collection=\"animal-facts\", id=\"info2\", text=\"Whales are mammals.\")\n",
    "await kernel.memory.save_information(collection=\"animal-facts\", id=\"info3\", text=\"Penguins are birds.\")\n",
    "await kernel.memory.save_information(collection=\"animal-facts\", id=\"info4\", text=\"Dolphins are mammals.\")\n",
    "await kernel.memory.save_information(collection=\"animal-facts\", id=\"info5\", text=\"Flies are insects.\")\n",
    "\n",
    "# Define prompt function using SK prompt template language\n",
    "my_prompt = \"\"\"I know these animal facts: {{recall $query1}} {{recall $query2}} {{recall $query3}} and \"\"\"\n",
    "\n",
    "execution_settings = sk_hf.HuggingFacePromptExecutionSettings(\n",
    "    service_id=text_service_id,\n",
    "    ai_model_id=text_service_id,\n",
    "    max_tokens=45,\n",
    "    temperature=0.5,\n",
    "    top_p=0.5,\n",
    ")\n",
    "\n",
    "prompt_template_config = sk.PromptTemplateConfig(\n",
    "    template=my_prompt,\n",
    "    name=\"text_complete\",\n",
    "    template_format=\"semantic-kernel\",\n",
    "    execution_settings=execution_settings,\n",
    ")\n",
    "\n",
    "my_function = kernel.create_function_from_prompt(prompt_template_config=prompt_template_config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2calf857",
   "metadata": {},
   "source": [
    "Let's now see what the completion looks like! Remember, \"gpt2\" is nowhere near as large as ChatGPT, so expect a much simpler answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "628c843e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/evmattso/.cache/pypoetry/virtualenvs/semantic-kernel-eoLGgW5m-py3.10/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/evmattso/.cache/pypoetry/virtualenvs/semantic-kernel-eoLGgW5m-py3.10/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/evmattso/.cache/pypoetry/virtualenvs/semantic-kernel-eoLGgW5m-py3.10/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/evmattso/.cache/pypoetry/virtualenvs/semantic-kernel-eoLGgW5m-py3.10/lib/python3.10/site-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 21, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Error occurred while invoking function f_HSMshmxNdvxpFZSl: ('Hugging Face completion failed', ValueError('If `eos_token_id` is defined, make sure that `pad_token_id` is defined.'), None)\n",
      "Error occurred while invoking function f_HSMshmxNdvxpFZSl: ('Hugging Face completion failed', ValueError('If `eos_token_id` is defined, make sure that `pad_token_id` is defined.'), None)\n"
     ]
    },
    "tags": []
   },
   "source": [
    "# Using Hugging Face With Plugins\n",
    "\n",
    "In this notebook, we demonstrate using Hugging Face models for Plugins using both SemanticMemory and text completions. \n",
    "\n",
    "SK supports downloading models from the Hugging Face that can perform the following tasks: text-generation, text2text-generation, summarization, and sentence-similarity. You can search for models by task at https://huggingface.co/models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a77bdf89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: semantic-kernel==0.9.1b1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (0.9.1b1)\n",
      "Requirement already satisfied: aiofiles<24.0.0,>=23.1.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from semantic-kernel==0.9.1b1) (23.2.1)\n",
      "Requirement already satisfied: aiohttp<4.0,>=3.8 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from semantic-kernel==0.9.1b1) (3.9.5)\n",
      "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /home/codespace/.local/lib/python3.10/site-packages (from semantic-kernel==0.9.1b1) (0.7.1)\n",
      "Requirement already satisfied: motor<4.0.0,>=3.3.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from semantic-kernel==0.9.1b1) (3.4.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.24.2 in /home/codespace/.local/lib/python3.10/site-packages (from semantic-kernel==0.9.1b1) (1.26.4)\n",
      "Requirement already satisfied: openai>=1.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from semantic-kernel==0.9.1b1) (1.33.0)\n",
      "Requirement already satisfied: openapi_core<0.19.0,>=0.18.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from semantic-kernel==0.9.1b1) (0.18.2)\n",
      "Requirement already satisfied: prance<24.0.0.0,>=23.6.21.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from semantic-kernel==0.9.1b1) (23.6.21.0)\n",
      "Requirement already satisfied: pydantic>2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from semantic-kernel==0.9.1b1) (2.7.3)\n",
      "Requirement already satisfied: python-dotenv==1.0.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from semantic-kernel==0.9.1b1) (1.0.1)\n",
      "Requirement already satisfied: regex<2024.0.0,>=2023.6.3 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from semantic-kernel==0.9.1b1) (2023.12.25)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from aiohttp<4.0,>=3.8->semantic-kernel==0.9.1b1) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/codespace/.local/lib/python3.10/site-packages (from aiohttp<4.0,>=3.8->semantic-kernel==0.9.1b1) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from aiohttp<4.0,>=3.8->semantic-kernel==0.9.1b1) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from aiohttp<4.0,>=3.8->semantic-kernel==0.9.1b1) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from aiohttp<4.0,>=3.8->semantic-kernel==0.9.1b1) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from aiohttp<4.0,>=3.8->semantic-kernel==0.9.1b1) (4.0.3)\n",
      "Requirement already satisfied: pymongo<5,>=4.5 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from motor<4.0.0,>=3.3.2->semantic-kernel==0.9.1b1) (4.7.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/codespace/.local/lib/python3.10/site-packages (from openai>=1.0->semantic-kernel==0.9.1b1) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openai>=1.0->semantic-kernel==0.9.1b1) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/codespace/.local/lib/python3.10/site-packages (from openai>=1.0->semantic-kernel==0.9.1b1) (0.27.0)\n",
      "Requirement already satisfied: sniffio in /home/codespace/.local/lib/python3.10/site-packages (from openai>=1.0->semantic-kernel==0.9.1b1) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openai>=1.0->semantic-kernel==0.9.1b1) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/codespace/.local/lib/python3.10/site-packages (from openai>=1.0->semantic-kernel==0.9.1b1) (4.10.0)\n",
      "Requirement already satisfied: asgiref<4.0.0,>=3.6.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.1b1) (3.8.1)\n",
      "Requirement already satisfied: isodate in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.1b1) (0.6.1)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.18.0 in /home/codespace/.local/lib/python3.10/site-packages (from openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.1b1) (4.21.1)\n",
      "Requirement already satisfied: jsonschema-spec<0.3.0,>=0.2.3 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.1b1) (0.2.4)\n",
      "Requirement already satisfied: more-itertools in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.1b1) (10.2.0)\n",
      "Requirement already satisfied: openapi-schema-validator<0.7.0,>=0.6.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.1b1) (0.6.2)\n",
      "Requirement already satisfied: openapi-spec-validator<0.8.0,>=0.7.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.1b1) (0.7.1)\n",
      "Requirement already satisfied: parse in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.1b1) (1.20.1)\n",
      "Requirement already satisfied: werkzeug in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.1b1) (3.0.3)\n",
      "Requirement already satisfied: chardet>=3.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from prance<24.0.0.0,>=23.6.21.0->semantic-kernel==0.9.1b1) (5.2.0)\n",
      "Requirement already satisfied: ruamel.yaml>=0.17.10 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from prance<24.0.0.0,>=23.6.21.0->semantic-kernel==0.9.1b1) (0.18.6)\n",
      "Requirement already satisfied: requests>=2.25 in /home/codespace/.local/lib/python3.10/site-packages (from prance<24.0.0.0,>=23.6.21.0->semantic-kernel==0.9.1b1) (2.31.0)\n",
      "Requirement already satisfied: six~=1.15 in /home/codespace/.local/lib/python3.10/site-packages (from prance<24.0.0.0,>=23.6.21.0->semantic-kernel==0.9.1b1) (1.16.0)\n",
      "Requirement already satisfied: packaging>=21.3 in /home/codespace/.local/lib/python3.10/site-packages (from prance<24.0.0.0,>=23.6.21.0->semantic-kernel==0.9.1b1) (23.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pydantic>2->semantic-kernel==0.9.1b1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pydantic>2->semantic-kernel==0.9.1b1) (2.18.4)\n",
      "Requirement already satisfied: idna>=2.8 in /home/codespace/.local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai>=1.0->semantic-kernel==0.9.1b1) (3.6)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/codespace/.local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai>=1.0->semantic-kernel==0.9.1b1) (1.2.0)\n",
      "Requirement already satisfied: certifi in /home/codespace/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai>=1.0->semantic-kernel==0.9.1b1) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai>=1.0->semantic-kernel==0.9.1b1) (1.0.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/codespace/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.0->semantic-kernel==0.9.1b1) (0.14.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.1b1) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.1b1) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/codespace/.local/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.1b1) (0.18.0)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /home/codespace/.local/lib/python3.10/site-packages (from jsonschema-spec<0.3.0,>=0.2.3->openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.1b1) (6.0.1)\n",
      "Requirement already satisfied: pathable<0.5.0,>=0.4.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from jsonschema-spec<0.3.0,>=0.2.3->openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.1b1) (0.4.3)\n",
      "Requirement already satisfied: rfc3339-validator in /home/codespace/.local/lib/python3.10/site-packages (from openapi-schema-validator<0.7.0,>=0.6.0->openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.1b1) (0.1.4)\n",
      "Requirement already satisfied: jsonschema-path<0.4.0,>=0.3.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openapi-spec-validator<0.8.0,>=0.7.1->openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.1b1) (0.3.2)\n",
      "Requirement already satisfied: lazy-object-proxy<2.0.0,>=1.7.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openapi-spec-validator<0.8.0,>=0.7.1->openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.1b1) (1.10.0)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pymongo<5,>=4.5->motor<4.0.0,>=3.3.2->semantic-kernel==0.9.1b1) (2.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests>=2.25->prance<24.0.0.0,>=23.6.21.0->semantic-kernel==0.9.1b1) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from requests>=2.25->prance<24.0.0.0,>=23.6.21.0->semantic-kernel==0.9.1b1) (2.0.7)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from ruamel.yaml>=0.17.10->prance<24.0.0.0,>=23.6.21.0->semantic-kernel==0.9.1b1) (0.2.8)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/codespace/.local/lib/python3.10/site-packages (from werkzeug->openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.1b1) (2.1.5)\n",
      "Collecting torch==2.0.0\n",
      "  Downloading torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.0.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.0.0) (4.10.0)\n",
      "Requirement already satisfied: sympy in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.0.0) (1.12)\n",
      "Requirement already satisfied: networkx in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.0.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.0.0) (3.1.3)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.0)\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.0)\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.0)\n",
      "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.0)\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.0)\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.0)\n",
      "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.0)\n",
      "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.0)\n",
      "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.0)\n",
      "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.0)\n",
      "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.0)\n",
      "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==2.0.0 (from torch==2.0.0)\n",
      "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/python/3.10.13/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0) (68.2.2)\n",
      "Collecting wheel (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0)\n",
      "  Using cached wheel-0.43.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting cmake (from triton==2.0.0->torch==2.0.0)\n",
      "  Downloading cmake-3.29.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting lit (from triton==2.0.0->torch==2.0.0)\n",
      "  Downloading lit-18.1.7-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.10/site-packages (from jinja2->torch==2.0.0) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/codespace/.local/lib/python3.10/site-packages (from sympy->torch==2.0.0) (1.3.0)\n",
      "Downloading torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading cmake-3.29.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading lit-18.1.7-py3-none-any.whl (96 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached wheel-0.43.0-py3-none-any.whl (65 kB)\n",
      "Installing collected packages: lit, wheel, nvidia-nccl-cu11, nvidia-cufft-cu11, nvidia-cuda-nvrtc-cu11, cmake, nvidia-nvtx-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch\n",
      "\u001b[33m  WARNING: The script lit is installed in '/usr/local/python/3.10.13/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  NOTE: The current PATH contains path(s) starting with `~`, which may not be expanded by all applications.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script wheel is installed in '/usr/local/python/3.10.13/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  NOTE: The current PATH contains path(s) starting with `~`, which may not be expanded by all applications.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts cmake, cpack and ctest are installed in '/usr/local/python/3.10.13/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  NOTE: The current PATH contains path(s) starting with `~`, which may not be expanded by all applications.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.2.0\n",
      "    Uninstalling triton-2.2.0:\n",
      "      Successfully uninstalled triton-2.2.0\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.2.1\n",
      "    Uninstalling torch-2.2.1:\n",
      "      Successfully uninstalled torch-2.2.1\n",
      "\u001b[33m  WARNING: The scripts convert-caffe2-to-onnx, convert-onnx-to-caffe2 and torchrun are installed in '/usr/local/python/3.10.13/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  NOTE: The current PATH contains path(s) starting with `~`, which may not be expanded by all applications.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed cmake-3.29.5 lit-18.1.7 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.0 triton-2.0.0 wheel-0.43.0\n",
      "\u001b[31mERROR: Ignored the following yanked versions: 4.14.0, 4.25.0\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement transformers==^4.28.1 (from versions: 0.1, 2.0.0, 2.1.0, 2.1.1, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0, 2.7.0, 2.8.0, 2.9.0, 2.9.1, 2.10.0, 2.11.0, 3.0.0, 3.0.1, 3.0.2, 3.1.0, 3.2.0, 3.3.0, 3.3.1, 3.4.0, 3.5.0, 3.5.1, 4.0.0rc1, 4.0.0, 4.0.1, 4.1.0, 4.1.1, 4.2.0, 4.2.1, 4.2.2, 4.3.0rc1, 4.3.0, 4.3.1, 4.3.2, 4.3.3, 4.4.0, 4.4.1, 4.4.2, 4.5.0, 4.5.1, 4.6.0, 4.6.1, 4.7.0, 4.8.0, 4.8.1, 4.8.2, 4.9.0, 4.9.1, 4.9.2, 4.10.0, 4.10.1, 4.10.2, 4.10.3, 4.11.0, 4.11.1, 4.11.2, 4.11.3, 4.12.0, 4.12.1, 4.12.2, 4.12.3, 4.12.4, 4.12.5, 4.13.0, 4.14.1, 4.15.0, 4.16.0, 4.16.1, 4.16.2, 4.17.0, 4.18.0, 4.19.0, 4.19.1, 4.19.2, 4.19.3, 4.19.4, 4.20.0, 4.20.1, 4.21.0, 4.21.1, 4.21.2, 4.21.3, 4.22.0, 4.22.1, 4.22.2, 4.23.0, 4.23.1, 4.24.0, 4.25.1, 4.26.0, 4.26.1, 4.27.0, 4.27.1, 4.27.2, 4.27.3, 4.27.4, 4.28.0, 4.28.1, 4.29.0, 4.29.1, 4.29.2, 4.30.0, 4.30.1, 4.30.2, 4.31.0, 4.32.0, 4.32.1, 4.33.0, 4.33.1, 4.33.2, 4.33.3, 4.34.0, 4.34.1, 4.35.0, 4.35.1, 4.35.2, 4.36.0, 4.36.1, 4.36.2, 4.37.0, 4.37.1, 4.37.2, 4.38.0, 4.38.1, 4.38.2, 4.39.0, 4.39.1, 4.39.2, 4.39.3, 4.40.0, 4.40.1, 4.40.2, 4.41.0, 4.41.1, 4.41.2)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for transformers==^4.28.1\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Ignored the following yanked versions: 0.2.6\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement sentence-transformers==^2.2.2 (from versions: 0.1.0, 0.2.0, 0.2.1, 0.2.2, 0.2.3, 0.2.4, 0.2.4.1, 0.2.5, 0.2.5.1, 0.2.6.1, 0.2.6.2, 0.3.0, 0.3.1, 0.3.2, 0.3.3, 0.3.4, 0.3.5, 0.3.5.1, 0.3.6, 0.3.7, 0.3.7.1, 0.3.7.2, 0.3.8, 0.3.9, 0.4.0, 0.4.1, 0.4.1.1, 0.4.1.2, 1.0.0, 1.0.1, 1.0.2, 1.0.3, 1.0.4, 1.1.0, 1.1.1, 1.2.0, 1.2.1, 2.0.0, 2.1.0, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.5.0, 2.5.1, 2.6.0, 2.6.1, 2.7.0, 3.0.0, 3.0.1)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for sentence-transformers==^2.2.2\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m pip install semantic-kernel==0.9.1b1\n",
    "\n",
    "# Note that additional dependencies are required for the Hugging Face connectors:\n",
    "!python -m pip install torch==2.0.0\n",
    "!python -m pip install transformers==^4.28.1\n",
    "!python -m pip install sentence-transformers==^2.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "508ad44f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msemantic_kernel\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msk\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msemantic_kernel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnectors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhugging_face\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msk_hf\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msemantic_kernel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmemory\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msemantic_text_memory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SemanticTextMemory\n",
      "File \u001b[0;32m/usr/local/python/3.10.13/lib/python3.10/site-packages/semantic_kernel/connectors/ai/hugging_face/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) Microsoft. All rights reserved.\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msemantic_kernel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnectors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhugging_face\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhf_prompt_execution_settings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      4\u001b[0m     HuggingFacePromptExecutionSettings,\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msemantic_kernel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnectors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhugging_face\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhf_text_completion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     HuggingFaceTextCompletion,\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msemantic_kernel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnectors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhugging_face\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhf_text_embedding\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     10\u001b[0m     HuggingFaceTextEmbedding,\n\u001b[1;32m     11\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/python/3.10.13/lib/python3.10/site-packages/semantic_kernel/connectors/ai/hugging_face/hf_prompt_execution_settings.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Dict\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GenerationConfig\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msemantic_kernel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnectors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompt_execution_settings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PromptExecutionSettings\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mHuggingFacePromptExecutionSettings\u001b[39;00m(PromptExecutionSettings):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
      "gpt2 completed prompt with: ''\n"
     ]
    }
   ],
   "source": [
    "import semantic_kernel as sk\n",
    "import semantic_kernel.connectors.ai.hugging_face as sk_hf\n",
    "from semantic_kernel.memory.semantic_text_memory import SemanticTextMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753ab756",
   "metadata": {},
   "outputs": [],
   "source": [
    "from services import Service\n",
    "\n",
    "# Select a service to use for this notebook (available services: OpenAI, AzureOpenAI, HuggingFace)\n",
    "selectedService = Service.HuggingFace"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8ddffc1",
   "metadata": {},
   "source": [
    "First, we will create a kernel and add both text completion and embedding services. \n",
    "\n",
    "For text completion, we are choosing GPT2. This is a text-generation model. (Note: text-generation will repeat the input in the output, text2text-generation will not.)\n",
    "For embeddings, we are using sentence-transformers/all-MiniLM-L6-v2. Vectors generated for this model are of length 384 (compared to a length of 1536 from OpenAI ADA).\n",
    "\n",
    "The following step may take a few minutes when run for the first time as the models will be downloaded to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8dcbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = sk.Kernel()\n",
    "\n",
    "# Configure LLM service\n",
    "if selectedService == Service.HuggingFace:\n",
    "    # Feel free to update this model to any other model available on Hugging Face\n",
    "    text_service_id = \"HuggingFaceM4/tiny-random-LlamaForCausalLM\"\n",
    "    kernel.add_service(\n",
    "        service=sk_hf.HuggingFaceTextCompletion(\n",
    "            service_id=text_service_id, ai_model_id=text_service_id, task=\"text-generation\"\n",
    "        ),\n",
    "    )\n",
    "    embed_service_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    embedding_svc = sk_hf.HuggingFaceTextEmbedding(service_id=embed_service_id, ai_model_id=embed_service_id)\n",
    "    kernel.add_service(\n",
    "        service=embedding_svc,\n",
    "    )\n",
    "    memory = SemanticTextMemory(storage=sk.memory.VolatileMemoryStore(), embeddings_generator=embedding_svc)\n",
    "    kernel.import_plugin_from_object(sk.core_plugins.TextMemoryPlugin(memory), \"TextMemoryPlugin\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2a7e7ca4",
   "metadata": {},
   "source": [
    "### Add Memories and Define a plugin to use them\n",
    "\n",
    "Most models available on huggingface.co are not as powerful as OpenAI GPT-3+. Your plugins will likely need to be simpler to accommodate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d096504c",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_id = \"generic\"\n",
    "\n",
    "await memory.save_information(collection=collection_id, id=\"info1\", text=\"Sharks are fish.\")\n",
    "await memory.save_information(collection=collection_id, id=\"info2\", text=\"Whales are mammals.\")\n",
    "await memory.save_information(collection=collection_id, id=\"info3\", text=\"Penguins are birds.\")\n",
    "await memory.save_information(collection=collection_id, id=\"info4\", text=\"Dolphins are mammals.\")\n",
    "await memory.save_information(collection=collection_id, id=\"info5\", text=\"Flies are insects.\")\n",
    "\n",
    "# Define prompt function using SK prompt template language\n",
    "my_prompt = \"\"\"I know these animal facts: \n",
    "- {{recall 'fact about sharks'}}\n",
    "- {{recall 'fact about whales'}} \n",
    "- {{recall 'fact about penguins'}} \n",
    "- {{recall 'fact about dolphins'}} \n",
    "- {{recall 'fact about flies'}}\n",
    "Now, tell me something about: {{$request}}\"\"\"\n",
    "\n",
    "execution_settings = sk_hf.HuggingFacePromptExecutionSettings(\n",
    "    service_id=text_service_id,\n",
    "    ai_model_id=text_service_id,\n",
    "    max_tokens=45,\n",
    "    temperature=0.5,\n",
    "    top_p=0.5,\n",
    ")\n",
    "\n",
    "prompt_template_config = sk.PromptTemplateConfig(\n",
    "    template=my_prompt,\n",
    "    name=\"text_complete\",\n",
    "    template_format=\"semantic-kernel\",\n",
    "    execution_settings=execution_settings,\n",
    ")\n",
    "\n",
    "my_function = kernel.create_function_from_prompt(\n",
    "    function_name=\"text_complete\",\n",
    "    plugin_name=\"TextCompletionPlugin\",\n",
    "    prompt_template_config=prompt_template_config,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2calf857",
   "metadata": {},
   "source": [
    "Let's now see what the completion looks like! Remember, \"gpt2\" is nowhere near as large as ChatGPT, so expect a much simpler answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628c843e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = await kernel.invoke(\n",
    "    my_function,\n",
    "    request=\"What are whales?\",\n",
    "from semantic_kernel.functions import KernelArguments\n",
    "\n",
    "arguments = KernelArguments(\n",
    "    query1=\"animal that swims\",\n",
    "    query2=\"animal that flies\",\n",
    "    query3=\"penguins are?\",\n",
    "    collection=\"animal-facts\",\n",
    "    relevance=0.3,\n",
    ")\n",
    "\n",
    "output = await kernel.invoke(\n",
    "    my_function,\n",
    "    arguments,\n",
    ")\n",
    "\n",
    "output = str(output).strip()\n",
    "\n",
    "query_result1 = await memory.search(\n",
    "    collection=collection_id, query=\"What are sharks?\", limit=1, min_relevance_score=0.3\n",
    ")\n",
    "\n",
    "print(f\"The queried result for 'What are sharks?' is {query_result1[0].text}\")\n",
    "query_result1 = await kernel.memory.search(\n",
    "    collection=\"animal-facts\", query=arguments[\"query1\"], limit=1, min_relevance_score=0.3\n",
    ")\n",
    "query_result2 = await kernel.memory.search(\n",
    "    collection=\"animal-facts\", query=arguments[\"query2\"], limit=1, min_relevance_score=0.3\n",
    ")\n",
    "query_result3 = await kernel.memory.search(\n",
    "    collection=\"animal-facts\", query=arguments[\"query3\"], limit=1, min_relevance_score=0.3\n",
    ")\n",
    "\n",
    "print(f\"{text_service_id} completed prompt with: '{output}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
