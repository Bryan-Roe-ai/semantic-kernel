{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "68e1c158",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Multiple Results\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb81bacd",
   "metadata": {},
   "source": [
    "In this notebook we show how you can in a single request, have the LLM model return multiple results per prompt. This is useful for running experiments where you want to evaluate the robustness of your prompt and the parameters of your config against a particular large language model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a77bdf89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: semantic-kernel==0.9.1b1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (0.9.1b1)\n",
      "Requirement already satisfied: aiofiles<24.0.0,>=23.1.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from semantic-kernel==0.9.1b1) (23.2.1)\n",
      "Requirement already satisfied: aiohttp<4.0,>=3.8 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from semantic-kernel==0.9.1b1) (3.9.5)\n",
      "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /home/codespace/.local/lib/python3.10/site-packages (from semantic-kernel==0.9.1b1) (0.7.1)\n",
      "Requirement already satisfied: motor<4.0.0,>=3.3.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from semantic-kernel==0.9.1b1) (3.4.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.24.2 in /home/codespace/.local/lib/python3.10/site-packages (from semantic-kernel==0.9.1b1) (1.26.4)\n",
      "Requirement already satisfied: openai>=1.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from semantic-kernel==0.9.1b1) (1.33.0)\n",
      "Requirement already satisfied: openapi_core<0.19.0,>=0.18.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from semantic-kernel==0.9.1b1) (0.18.2)\n",
      "Requirement already satisfied: prance<24.0.0.0,>=23.6.21.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from semantic-kernel==0.9.1b1) (23.6.21.0)\n",
      "Requirement already satisfied: pydantic>2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from semantic-kernel==0.9.1b1) (2.7.3)\n",
      "Requirement already satisfied: python-dotenv==1.0.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from semantic-kernel==0.9.1b1) (1.0.1)\n",
      "Requirement already satisfied: regex<2024.0.0,>=2023.6.3 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from semantic-kernel==0.9.1b1) (2023.12.25)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from aiohttp<4.0,>=3.8->semantic-kernel==0.9.1b1) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/codespace/.local/lib/python3.10/site-packages (from aiohttp<4.0,>=3.8->semantic-kernel==0.9.1b1) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from aiohttp<4.0,>=3.8->semantic-kernel==0.9.1b1) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from aiohttp<4.0,>=3.8->semantic-kernel==0.9.1b1) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from aiohttp<4.0,>=3.8->semantic-kernel==0.9.1b1) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from aiohttp<4.0,>=3.8->semantic-kernel==0.9.1b1) (4.0.3)\n",
      "Requirement already satisfied: pymongo<5,>=4.5 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from motor<4.0.0,>=3.3.2->semantic-kernel==0.9.1b1) (4.7.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/codespace/.local/lib/python3.10/site-packages (from openai>=1.0->semantic-kernel==0.9.1b1) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openai>=1.0->semantic-kernel==0.9.1b1) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/codespace/.local/lib/python3.10/site-packages (from openai>=1.0->semantic-kernel==0.9.1b1) (0.27.0)\n",
      "Requirement already satisfied: sniffio in /home/codespace/.local/lib/python3.10/site-packages (from openai>=1.0->semantic-kernel==0.9.1b1) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openai>=1.0->semantic-kernel==0.9.1b1) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/codespace/.local/lib/python3.10/site-packages (from openai>=1.0->semantic-kernel==0.9.1b1) (4.10.0)\n",
      "Requirement already satisfied: asgiref<4.0.0,>=3.6.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.1b1) (3.8.1)\n",
      "Requirement already satisfied: isodate in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.1b1) (0.6.1)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.18.0 in /home/codespace/.local/lib/python3.10/site-packages (from openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.1b1) (4.21.1)\n",
      "Requirement already satisfied: jsonschema-spec<0.3.0,>=0.2.3 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.1b1) (0.2.4)\n",
      "Requirement already satisfied: more-itertools in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.1b1) (10.2.0)\n",
      "Requirement already satisfied: openapi-schema-validator<0.7.0,>=0.6.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.1b1) (0.6.2)\n",
      "Requirement already satisfied: openapi-spec-validator<0.8.0,>=0.7.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.1b1) (0.7.1)\n",
      "Requirement already satisfied: parse in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.1b1) (1.20.1)\n",
      "Requirement already satisfied: werkzeug in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.1b1) (3.0.3)\n",
      "Requirement already satisfied: chardet>=3.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from prance<24.0.0.0,>=23.6.21.0->semantic-kernel==0.9.1b1) (5.2.0)\n",
      "Requirement already satisfied: ruamel.yaml>=0.17.10 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from prance<24.0.0.0,>=23.6.21.0->semantic-kernel==0.9.1b1) (0.18.6)\n",
      "Requirement already satisfied: requests>=2.25 in /home/codespace/.local/lib/python3.10/site-packages (from prance<24.0.0.0,>=23.6.21.0->semantic-kernel==0.9.1b1) (2.31.0)\n",
      "Requirement already satisfied: six~=1.15 in /home/codespace/.local/lib/python3.10/site-packages (from prance<24.0.0.0,>=23.6.21.0->semantic-kernel==0.9.1b1) (1.16.0)\n",
      "Requirement already satisfied: packaging>=21.3 in /home/codespace/.local/lib/python3.10/site-packages (from prance<24.0.0.0,>=23.6.21.0->semantic-kernel==0.9.1b1) (23.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pydantic>2->semantic-kernel==0.9.1b1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pydantic>2->semantic-kernel==0.9.1b1) (2.18.4)\n",
      "Requirement already satisfied: idna>=2.8 in /home/codespace/.local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai>=1.0->semantic-kernel==0.9.1b1) (3.6)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/codespace/.local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai>=1.0->semantic-kernel==0.9.1b1) (1.2.0)\n",
      "Requirement already satisfied: certifi in /home/codespace/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai>=1.0->semantic-kernel==0.9.1b1) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai>=1.0->semantic-kernel==0.9.1b1) (1.0.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/codespace/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.0->semantic-kernel==0.9.1b1) (0.14.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.1b1) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.1b1) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/codespace/.local/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.1b1) (0.18.0)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /home/codespace/.local/lib/python3.10/site-packages (from jsonschema-spec<0.3.0,>=0.2.3->openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.1b1) (6.0.1)\n",
      "Requirement already satisfied: pathable<0.5.0,>=0.4.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from jsonschema-spec<0.3.0,>=0.2.3->openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.1b1) (0.4.3)\n",
      "Requirement already satisfied: rfc3339-validator in /home/codespace/.local/lib/python3.10/site-packages (from openapi-schema-validator<0.7.0,>=0.6.0->openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.1b1) (0.1.4)\n",
      "Requirement already satisfied: jsonschema-path<0.4.0,>=0.3.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openapi-spec-validator<0.8.0,>=0.7.1->openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.1b1) (0.3.2)\n",
      "Requirement already satisfied: lazy-object-proxy<2.0.0,>=1.7.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openapi-spec-validator<0.8.0,>=0.7.1->openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.1b1) (1.10.0)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pymongo<5,>=4.5->motor<4.0.0,>=3.3.2->semantic-kernel==0.9.1b1) (2.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests>=2.25->prance<24.0.0.0,>=23.6.21.0->semantic-kernel==0.9.1b1) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from requests>=2.25->prance<24.0.0.0,>=23.6.21.0->semantic-kernel==0.9.1b1) (2.0.7)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from ruamel.yaml>=0.17.10->prance<24.0.0.0,>=23.6.21.0->semantic-kernel==0.9.1b1) (0.2.8)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/codespace/.local/lib/python3.10/site-packages (from werkzeug->openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.1b1) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install semantic-kernel==0.9.1b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f4bfee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from services import Service\n",
    "\n",
    "# Select a service to use for this notebook (available services: OpenAI, AzureOpenAI, HuggingFace)\n",
    "selectedService = Service.OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "508ad44f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'semantic_kernel.models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msemantic_kernel\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msk\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msemantic_kernel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_completion\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_history\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatHistory\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m selectedService \u001b[38;5;241m==\u001b[39m Service\u001b[38;5;241m.\u001b[39mOpenAI \u001b[38;5;129;01mor\u001b[39;00m selectedService \u001b[38;5;241m==\u001b[39m Service\u001b[38;5;241m.\u001b[39mAzureOpenAI:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msemantic_kernel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnectors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopen_ai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompt_execution_settings\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopen_ai_prompt_execution_settings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      6\u001b[0m         OpenAITextPromptExecutionSettings,\n\u001b[1;32m      7\u001b[0m         OpenAIChatPromptExecutionSettings,\n\u001b[1;32m      8\u001b[0m     )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'semantic_kernel.models'"
     ]
    }
   ],
   "source": [
    "import semantic_kernel as sk\n",
    "from semantic_kernel.models.ai.chat_completion.chat_history import ChatHistory\n",
    "\n",
    "if selectedService == Service.OpenAI or selectedService == Service.AzureOpenAI:\n",
    "    from semantic_kernel.connectors.ai.open_ai.prompt_execution_settings.open_ai_prompt_execution_settings import (\n",
    "        OpenAITextPromptExecutionSettings,\n",
    "        OpenAIChatPromptExecutionSettings,\n",
    "    )\n",
    "    from semantic_kernel.connectors.ai.open_ai.prompt_execution_settings.azure_chat_prompt_execution_settings import (\n",
    "        AzureChatPromptExecutionSettings,\n",
    "    )\n",
    "    from semantic_kernel.connectors.ai.open_ai import (\n",
    "        AzureTextCompletion,\n",
    "        AzureChatCompletion,\n",
    "        OpenAITextCompletion,\n",
    "        OpenAIChatCompletion,\n",
    "    )\n",
    "if selectedService == Service.HuggingFace:\n",
    "    from semantic_kernel.connectors.ai.hugging_face import HuggingFaceTextCompletion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8ddffc1",
   "metadata": {},
   "source": [
    "First, we will set up the text and chat services we will be submitting prompts to.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8dcbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = sk.Kernel()\n",
    "\n",
    "# Configure Azure LLM service\n",
    "if selectedService == Service.AzureOpenAI:\n",
    "    deployment, api_key, endpoint = sk.azure_openai_settings_from_dot_env()\n",
    "    azure_text_service = AzureTextCompletion(\n",
    "        service_id=\"aoai_text\", deployment_name=\"gpt-35-turbo-instruct\", endpoint=endpoint, api_key=api_key\n",
    "    )  # set the deployment name to the value of your text model (e.g. gpt-35-turbo-instruct or text-davinci-003)\n",
    "    azure_chat_service = AzureChatCompletion(\n",
    "        service_id=\"aoai_chat\", deployment_name=\"gpt-35-turbo\", endpoint=endpoint, api_key=api_key\n",
    "    )  # set the deployment name to the value of your chat model\n",
    "\n",
    "# Configure OpenAI service\n",
    "if selectedService == Service.OpenAI:\n",
    "    api_key, org_id = sk.openai_settings_from_dot_env()\n",
    "    oai_text_service = OpenAITextCompletion(\n",
    "        service_id=\"oai_text\", ai_model_id=\"gpt-3.5-turbo-instruct\", api_key=api_key, org_id=org_id\n",
    "    )\n",
    "    oai_chat_service = OpenAIChatCompletion(\n",
    "        service_id=\"oai_chat\", ai_model_id=\"gpt-3.5-turbo\", api_key=api_key, org_id=org_id\n",
    "    )\n",
    "\n",
    "# Configure Hugging Face service\n",
    "if selectedService == Service.HuggingFace:\n",
    "    hf_text_service = HuggingFaceTextCompletion(service_id=\"hf_text\", ai_model_id=\"distilgpt2\", task=\"text-generation\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "50561d82",
   "metadata": {},
   "source": [
    "Next, we'll set up the completion request settings for text completion services.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628c843e",
   "metadata": {},
   "outputs": [],
   "source": [
    "oai_text_prompt_execution_settings = OpenAITextPromptExecutionSettings(\n",
    "    service=\"oai_text\",\n",
    "    extension_data={\n",
    "        \"max_tokens\": 80,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 1,\n",
    "        \"frequency_penalty\": 0.5,\n",
    "        \"presence_penalty\": 0.5,\n",
    "        \"number_of_responses\": 3,\n",
    "    },\n",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< head
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
=======
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
<<<<<<< main
=======
=======
>>>>>>> origin/main
=======
<<<<<<< main
=======
>>>>>>> Stashed changes
=======
<<<<<<< main
=======
>>>>>>> Stashed changes
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "857a9c89",
   "metadata": {},
   "source": [
    "## Multiple Open AI Text Completions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2979db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if selectedService == Service.OpenAI:\n",
    "    chat = ChatHistory()\n",
    "    chat.add_user_message(\"What is the purpose of a rubber duck?\")\n",
    "    results = await oai_text_service.complete(chat_history=chat, settings=oai_text_prompt_execution_settings)\n",
    "    i = 1\n",
    "    for result in results:\n",
    "        print(f\"Result {i}: {result}\")\n",
    "        i += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4288d09f",
   "metadata": {},
   "source": [
    "## Multiple Azure Open AI Text Completions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5319f14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if selectedService == Service.AzureOpenAI:\n",
    "    chat = ChatHistory()\n",
    "    chat.add_user_message(\"provide me a list of possible meanings for the acronym 'ORLD'\")\n",
    "    results = await azure_text_service.complete(chat_history=chat, settings=oai_text_prompt_execution_settings)\n",
    "    i = 1\n",
    "    for result in results:\n",
    "        print(f\"Result {i}: {result}\")\n",
    "        i += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb548f9c",
   "metadata": {},
   "source": [
    "## Multiple Hugging Face Text Completions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a148709",
   "metadata": {},
   "outputs": [],
   "source": [
    "if selectedService == Service.HuggingFace:\n",
    "    from semantic_kernel.connectors.ai.hugging_face.hf_prompt_execution_settings import (\n",
    "        HuggingFacePromptExecutionSettings,\n",
    "    )\n",
    "\n",
    "    hf_prompt_execution_settings = HuggingFacePromptExecutionSettings(\n",
    "        service_id=\"hf_text\", extension_data={\"max_new_tokens\": 80, \"temperature\": 0.7, \"top_p\": 1}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9525e4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if selectedService == Service.HuggingFace:\n",
    "    prompt = \"The purpose of a rubber duck is\"\n",
    "    chat = ChatHistory()\n",
    "    chat.add_user_message(prompt)\n",
    "    results = await hf_text_service.complete(chat_history=chat, prompt_execution_settings=hf_prompt_execution_settings)\n",
    "    print(\"\".join(results))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da632e12",
   "metadata": {},
   "source": [
    "Here, we're setting up the settings for Chat completions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f11e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "oai_chat_prompt_execution_settings = OpenAIChatPromptExecutionSettings(\n",
    "    service_id=\"oai_chat\",\n",
    "    max_tokens=80,\n",
    "    temperature=0.7,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0.5,\n",
    "    presence_penalty=0.5,\n",
    "    number_of_responses=3,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d6bf238e",
   "metadata": {},
   "source": [
    "## Multiple OpenAI Chat Completions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabc6a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if selectedService == Service.OpenAI:\n",
    "    chat = ChatHistory()\n",
    "    chat.add_user_message(\n",
    "        \"It's a beautiful day outside, birds are singing, flowers are blooming. On days like these, kids like you...\"\n",
    "    )\n",
    "    results = await oai_chat_service.complete_chat(chat_history=chat, settings=oai_chat_prompt_execution_settings)\n",
    "    i = 0\n",
    "    for result in results:\n",
    "        print(f\"Result {i+1}: {str(result)}\")\n",
    "        i += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cdb8f740",
   "metadata": {},
   "source": [
    "## Multiple Azure OpenAI Chat Completions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ba4767",
   "metadata": {},
   "outputs": [],
   "source": [
    "az_oai_prompt_execution_settings = AzureChatPromptExecutionSettings(\n",
    "    service_id=\"aoai_chat\",\n",
    "    max_tokens=80,\n",
    "    temperature=0.7,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0.5,\n",
    "    presence_penalty=0.5,\n",
    "    number_of_responses=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74a64a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if selectedService == Service.AzureOpenAI:\n",
    "    content = \"Tomorow is going to be a great day, I can feel it. I'm going to wake up early, go for a run, and then...\"\n",
    "    chat = ChatHistory()\n",
    "    chat.add_user_message(content)\n",
    "    results = await azure_chat_service.complete_chat(chat_history=chat, settings=az_oai_prompt_execution_settings)\n",
    "    i = 0\n",
    "    for result in results:\n",
    "        print(f\"Result {i+1}: {str(result)}\")\n",
    "        i += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "98c8191d",
   "metadata": {},
   "source": [
    "## Streaming Multiple Results\n",
    "\n",
    "Here is an example pattern if you want to stream your multiple results. Note that this is not supported for Hugging Face text completions at this time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a37702",
   "metadata": {},
   "outputs": [],
   "source": [
    "if selectedService == Service.OpenAI:\n",
    "    import os\n",
    "    from IPython.display import clear_output\n",
    "    import time\n",
    "\n",
    "    # Determine the clear command based on OS\n",
    "    clear_command = \"cls\" if os.name == \"nt\" else \"clear\"\n",
    "\n",
    "    chat = ChatHistory()\n",
    "    chat.add_user_message(\"what is the purpose of a rubber duck?\")\n",
    "\n",
    "    stream = oai_text_service.complete_stream(chat_history=chat, settings=oai_text_prompt_execution_settings)\n",
    "    number_of_responses = oai_text_prompt_execution_settings.number_of_responses\n",
    "    texts = [\"\"] * number_of_responses\n",
    "\n",
    "    last_clear_time = time.time()\n",
    "    clear_interval = 0.5  # seconds\n",
    "\n",
    "    # Note: there are some quirks with displaying the output, which sometimes flashes and disappears.\n",
    "    # This could be influenced by a few factors specific to Jupyter notebooks and asynchronous processing.\n",
    "    # The following code attempts to buffer the results to avoid the output flashing on/off the screen.\n",
    "\n",
    "    async for results in stream:\n",
    "        current_time = time.time()\n",
    "\n",
    "        # Update texts with new results\n",
    "        for idx, result in enumerate(results):\n",
    "            if idx < number_of_responses:\n",
    "                texts[idx] += str(result)\n",
    "\n",
    "        # Clear and display output at intervals\n",
    "        if current_time - last_clear_time > clear_interval:\n",
    "            clear_output(wait=True)\n",
    "            for idx, text in enumerate(texts):\n",
    "                print(f\"Result {idx + 1}: {text}\")\n",
    "            last_clear_time = current_time\n",
    "\n",
    "    print(\"----------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
   "metadata": {},
   "source": [
    "# Multiple Results\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb81bacd",
   "metadata": {},
   "source": [
    "In this notebook we show how you can in a single request, have the LLM model return multiple results per prompt. This is useful for running experiments where you want to evaluate the robustness of your prompt and the parameters of your config against a particular large language model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a77bdf89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: semantic-kernel==0.9.1b1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (0.9.1b1)\n",
      "Requirement already satisfied: aiofiles<24.0.0,>=23.1.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from semantic-kernel==0.9.1b1) (23.2.1)\n",
      "Requirement already satisfied: aiohttp<4.0,>=3.8 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from semantic-kernel==0.9.1b1) (3.9.5)\n",
      "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /home/codespace/.local/lib/python3.10/site-packages (from semantic-kernel==0.9.1b1) (0.7.1)\n",
      "Requirement already satisfied: motor<4.0.0,>=3.3.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from semantic-kernel==0.9.1b1) (3.4.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.24.2 in /home/codespace/.local/lib/python3.10/site-packages (from semantic-kernel==0.9.1b1) (1.26.4)\n",
      "Requirement already satisfied: openai>=1.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from semantic-kernel==0.9.1b1) (1.33.0)\n",
      "Requirement already satisfied: openapi_core<0.19.0,>=0.18.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from semantic-kernel==0.9.1b1) (0.18.2)\n",
      "Requirement already satisfied: prance<24.0.0.0,>=23.6.21.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from semantic-kernel==0.9.1b1) (23.6.21.0)\n",
      "Requirement already satisfied: pydantic>2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from semantic-kernel==0.9.1b1) (2.7.3)\n",
      "Requirement already satisfied: python-dotenv==1.0.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from semantic-kernel==0.9.1b1) (1.0.1)\n",
      "Requirement already satisfied: regex<2024.0.0,>=2023.6.3 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from semantic-kernel==0.9.1b1) (2023.12.25)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from aiohttp<4.0,>=3.8->semantic-kernel==0.9.1b1) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/codespace/.local/lib/python3.10/site-packages (from aiohttp<4.0,>=3.8->semantic-kernel==0.9.1b1) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from aiohttp<4.0,>=3.8->semantic-kernel==0.9.1b1) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from aiohttp<4.0,>=3.8->semantic-kernel==0.9.1b1) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from aiohttp<4.0,>=3.8->semantic-kernel==0.9.1b1) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from aiohttp<4.0,>=3.8->semantic-kernel==0.9.1b1) (4.0.3)\n",
      "Requirement already satisfied: pymongo<5,>=4.5 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from motor<4.0.0,>=3.3.2->semantic-kernel==0.9.1b1) (4.7.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/codespace/.local/lib/python3.10/site-packages (from openai>=1.0->semantic-kernel==0.9.1b1) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openai>=1.0->semantic-kernel==0.9.1b1) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/codespace/.local/lib/python3.10/site-packages (from openai>=1.0->semantic-kernel==0.9.1b1) (0.27.0)\n",
      "Requirement already satisfied: sniffio in /home/codespace/.local/lib/python3.10/site-packages (from openai>=1.0->semantic-kernel==0.9.1b1) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openai>=1.0->semantic-kernel==0.9.1b1) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/codespace/.local/lib/python3.10/site-packages (from openai>=1.0->semantic-kernel==0.9.1b1) (4.10.0)\n",
      "Requirement already satisfied: asgiref<4.0.0,>=3.6.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.1b1) (3.8.1)\n",
      "Requirement already satisfied: isodate in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.1b1) (0.6.1)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.18.0 in /home/codespace/.local/lib/python3.10/site-packages (from openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.1b1) (4.21.1)\n",
      "Requirement already satisfied: jsonschema-spec<0.3.0,>=0.2.3 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.1b1) (0.2.4)\n",
      "Requirement already satisfied: more-itertools in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.1b1) (10.2.0)\n",
      "Requirement already satisfied: openapi-schema-validator<0.7.0,>=0.6.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.1b1) (0.6.2)\n",
      "Requirement already satisfied: openapi-spec-validator<0.8.0,>=0.7.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.1b1) (0.7.1)\n",
      "Requirement already satisfied: parse in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.1b1) (1.20.1)\n",
      "Requirement already satisfied: werkzeug in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.1b1) (3.0.3)\n",
      "Requirement already satisfied: chardet>=3.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from prance<24.0.0.0,>=23.6.21.0->semantic-kernel==0.9.1b1) (5.2.0)\n",
      "Requirement already satisfied: ruamel.yaml>=0.17.10 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from prance<24.0.0.0,>=23.6.21.0->semantic-kernel==0.9.1b1) (0.18.6)\n",
      "Requirement already satisfied: requests>=2.25 in /home/codespace/.local/lib/python3.10/site-packages (from prance<24.0.0.0,>=23.6.21.0->semantic-kernel==0.9.1b1) (2.31.0)\n",
      "Requirement already satisfied: six~=1.15 in /home/codespace/.local/lib/python3.10/site-packages (from prance<24.0.0.0,>=23.6.21.0->semantic-kernel==0.9.1b1) (1.16.0)\n",
      "Requirement already satisfied: packaging>=21.3 in /home/codespace/.local/lib/python3.10/site-packages (from prance<24.0.0.0,>=23.6.21.0->semantic-kernel==0.9.1b1) (23.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pydantic>2->semantic-kernel==0.9.1b1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pydantic>2->semantic-kernel==0.9.1b1) (2.18.4)\n",
      "Requirement already satisfied: idna>=2.8 in /home/codespace/.local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai>=1.0->semantic-kernel==0.9.1b1) (3.6)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/codespace/.local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai>=1.0->semantic-kernel==0.9.1b1) (1.2.0)\n",
      "Requirement already satisfied: certifi in /home/codespace/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai>=1.0->semantic-kernel==0.9.1b1) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai>=1.0->semantic-kernel==0.9.1b1) (1.0.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/codespace/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.0->semantic-kernel==0.9.1b1) (0.14.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.1b1) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.1b1) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/codespace/.local/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.1b1) (0.18.0)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /home/codespace/.local/lib/python3.10/site-packages (from jsonschema-spec<0.3.0,>=0.2.3->openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.1b1) (6.0.1)\n",
      "Requirement already satisfied: pathable<0.5.0,>=0.4.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from jsonschema-spec<0.3.0,>=0.2.3->openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.1b1) (0.4.3)\n",
      "Requirement already satisfied: rfc3339-validator in /home/codespace/.local/lib/python3.10/site-packages (from openapi-schema-validator<0.7.0,>=0.6.0->openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.1b1) (0.1.4)\n",
      "Requirement already satisfied: jsonschema-path<0.4.0,>=0.3.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openapi-spec-validator<0.8.0,>=0.7.1->openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.1b1) (0.3.2)\n",
      "Requirement already satisfied: lazy-object-proxy<2.0.0,>=1.7.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openapi-spec-validator<0.8.0,>=0.7.1->openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.1b1) (1.10.0)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pymongo<5,>=4.5->motor<4.0.0,>=3.3.2->semantic-kernel==0.9.1b1) (2.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests>=2.25->prance<24.0.0.0,>=23.6.21.0->semantic-kernel==0.9.1b1) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from requests>=2.25->prance<24.0.0.0,>=23.6.21.0->semantic-kernel==0.9.1b1) (2.0.7)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from ruamel.yaml>=0.17.10->prance<24.0.0.0,>=23.6.21.0->semantic-kernel==0.9.1b1) (0.2.8)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/codespace/.local/lib/python3.10/site-packages (from werkzeug->openapi_core<0.19.0,>=0.18.0->semantic-kernel==0.9.1b1) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install semantic-kernel==0.5.1.dev0"
    "!python -m pip install semantic-kernel==0.9.1b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f4bfee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from services import Service\n",
    "\n",
    "# Select a service to use for this notebook (available services: OpenAI, AzureOpenAI, HuggingFace)\n",
    "selectedService = Service.OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "508ad44f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'semantic_kernel.models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msemantic_kernel\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msk\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msemantic_kernel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_completion\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_history\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatHistory\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m selectedService \u001b[38;5;241m==\u001b[39m Service\u001b[38;5;241m.\u001b[39mOpenAI \u001b[38;5;129;01mor\u001b[39;00m selectedService \u001b[38;5;241m==\u001b[39m Service\u001b[38;5;241m.\u001b[39mAzureOpenAI:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msemantic_kernel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnectors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopen_ai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompt_execution_settings\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopen_ai_prompt_execution_settings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      6\u001b[0m         OpenAITextPromptExecutionSettings,\n\u001b[1;32m      7\u001b[0m         OpenAIChatPromptExecutionSettings,\n\u001b[1;32m      8\u001b[0m     )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'semantic_kernel.models'"
     ]
    }
   ],
   "source": [
    "import semantic_kernel as sk\n",
    "from semantic_kernel.models.ai.chat_completion.chat_history import ChatHistory\n",
    "\n",
    "if selectedService == Service.OpenAI or selectedService == Service.AzureOpenAI:\n",
    "    from semantic_kernel.connectors.ai.open_ai.prompt_execution_settings.open_ai_prompt_execution_settings import (\n",
    "        OpenAITextPromptExecutionSettings,\n",
    "        OpenAIChatPromptExecutionSettings,\n",
    "    )\n",
    "    from semantic_kernel.connectors.ai.open_ai.prompt_execution_settings.azure_chat_prompt_execution_settings import (\n",
    "        AzureChatPromptExecutionSettings,\n",
    "    )\n",
    "    from semantic_kernel.connectors.ai.open_ai import (\n",
    "        AzureTextCompletion,\n",
    "        AzureChatCompletion,\n",
    "        OpenAITextCompletion,\n",
    "        OpenAIChatCompletion,\n",
    "    )\n",
    "if selectedService == Service.HuggingFace:\n",
    "    from semantic_kernel.connectors.ai.hugging_face import HuggingFaceTextCompletion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8ddffc1",
   "metadata": {},
   "source": [
    "First, we will set up the text and chat services we will be submitting prompts to.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8dcbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = sk.Kernel()\n",
    "\n",
    "# Configure Azure LLM service\n",
    "if selectedService == Service.AzureOpenAI:\n",
    "    deployment, api_key, endpoint = sk.azure_openai_settings_from_dot_env()\n",
    "    azure_text_service = AzureTextCompletion(\n",
    "        service_id=\"aoai_text\", deployment_name=\"gpt-35-turbo-instruct\", endpoint=endpoint, api_key=api_key\n",
    "    )  # set the deployment name to the value of your text model (e.g. gpt-35-turbo-instruct or text-davinci-003)\n",
    "    azure_chat_service = AzureChatCompletion(\n",
    "        service_id=\"aoai_chat\", deployment_name=\"gpt-35-turbo\", endpoint=endpoint, api_key=api_key\n",
    "    )  # set the deployment name to the value of your chat model\n",
    "\n",
    "# Configure OpenAI service\n",
    "if selectedService == Service.OpenAI:\n",
    "    api_key, org_id = sk.openai_settings_from_dot_env()\n",
    "    oai_text_service = OpenAITextCompletion(\n",
    "        service_id=\"oai_text\", ai_model_id=\"gpt-3.5-turbo-instruct\", api_key=api_key, org_id=org_id\n",
    "    )\n",
    "    oai_chat_service = OpenAIChatCompletion(\n",
    "        service_id=\"oai_chat\", ai_model_id=\"gpt-3.5-turbo\", api_key=api_key, org_id=org_id\n",
    "    )\n",
    "\n",
    "# Configure Hugging Face service\n",
    "if selectedService == Service.HuggingFace:\n",
    "    hf_text_service = HuggingFaceTextCompletion(service_id=\"hf_text\", ai_model_id=\"distilgpt2\", task=\"text-generation\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "50561d82",
   "metadata": {},
   "source": [
    "Next, we'll set up the completion request settings for text completion services.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628c843e",
   "metadata": {},
   "outputs": [],
   "source": [
    "oai_text_prompt_execution_settings = OpenAITextPromptExecutionSettings(\n",
    "    service=\"oai_text\",\n",
    "    extension_data={\n",
    "        \"max_tokens\": 80,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 1,\n",
    "        \"frequency_penalty\": 0.5,\n",
    "        \"presence_penalty\": 0.5,\n",
    "        \"number_of_responses\": 3,\n",
    "    },\n",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< head
>>>>>>> origin/main
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
>>>>>>> origin/main
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "857a9c89",
   "metadata": {},
   "source": [
    "## Multiple Open AI Text Completions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2979db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if selectedService == Service.OpenAI:\n",
    "    chat = ChatHistory()\n",
    "    chat.add_user_message(\"What is the purpose of a rubber duck?\")\n",
    "    results = await oai_text_service.complete(chat_history=chat, settings=oai_text_prompt_execution_settings)\n",
    "    i = 1\n",
    "    for result in results:\n",
    "        print(f\"Result {i}: {result}\")\n",
    "        i += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4288d09f",
   "metadata": {},
   "source": [
    "## Multiple Azure Open AI Text Completions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5319f14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if selectedService == Service.AzureOpenAI:\n",
    "    chat = ChatHistory()\n",
    "    chat.add_user_message(\"provide me a list of possible meanings for the acronym 'ORLD'\")\n",
    "    results = await azure_text_service.complete(chat_history=chat, settings=oai_text_prompt_execution_settings)\n",
    "    i = 1\n",
    "    for result in results:\n",
    "        print(f\"Result {i}: {result}\")\n",
    "        i += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb548f9c",
   "metadata": {},
   "source": [
    "## Multiple Hugging Face Text Completions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a148709",
   "metadata": {},
   "outputs": [],
   "source": [
    "if selectedService == Service.HuggingFace:\n",
    "    from semantic_kernel.connectors.ai.hugging_face.hf_prompt_execution_settings import (\n",
    "        HuggingFacePromptExecutionSettings,\n",
    "    )\n",
    "\n",
    "    hf_prompt_execution_settings = HuggingFacePromptExecutionSettings(\n",
    "        service_id=\"hf_text\", extension_data={\"max_new_tokens\": 80, \"temperature\": 0.7, \"top_p\": 1}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9525e4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if selectedService == Service.HuggingFace:\n",
    "    prompt = \"The purpose of a rubber duck is\"\n",
    "    chat = ChatHistory()\n",
    "    chat.add_user_message(prompt)\n",
    "    results = await hf_text_service.complete(chat_history=chat, prompt_execution_settings=hf_prompt_execution_settings)\n",
    "    print(\"\".join(results))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da632e12",
   "metadata": {},
   "source": [
    "Here, we're setting up the settings for Chat completions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f11e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "oai_chat_prompt_execution_settings = OpenAIChatPromptExecutionSettings(\n",
    "    service_id=\"oai_chat\",\n",
    "    max_tokens=80,\n",
    "    temperature=0.7,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0.5,\n",
    "    presence_penalty=0.5,\n",
    "    number_of_responses=3,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d6bf238e",
   "metadata": {},
   "source": [
    "## Multiple OpenAI Chat Completions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabc6a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if selectedService == Service.OpenAI:\n",
    "    chat = ChatHistory()\n",
    "    chat.add_user_message(\n",
    "        \"It's a beautiful day outside, birds are singing, flowers are blooming. On days like these, kids like you...\"\n",
    "    )\n",
    "    results = await oai_chat_service.complete_chat(chat_history=chat, settings=oai_chat_prompt_execution_settings)\n",
    "    i = 0\n",
    "    for result in results:\n",
    "        print(f\"Result {i+1}: {str(result)}\")\n",
    "        i += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cdb8f740",
   "metadata": {},
   "source": [
    "## Multiple Azure OpenAI Chat Completions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ba4767",
   "metadata": {},
   "outputs": [],
   "source": [
    "az_oai_prompt_execution_settings = AzureChatPromptExecutionSettings(\n",
    "    service_id=\"aoai_chat\",\n",
    "    max_tokens=80,\n",
    "    temperature=0.7,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0.5,\n",
    "    presence_penalty=0.5,\n",
    "    number_of_responses=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74a64a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if selectedService == Service.AzureOpenAI:\n",
    "    content = \"Tomorow is going to be a great day, I can feel it. I'm going to wake up early, go for a run, and then...\"\n",
    "    chat = ChatHistory()\n",
    "    chat.add_user_message(content)\n",
    "    results = await azure_chat_service.complete_chat(chat_history=chat, settings=az_oai_prompt_execution_settings)\n",
    "    i = 0\n",
    "    for result in results:\n",
    "        print(f\"Result {i+1}: {str(result)}\")\n",
    "        i += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "98c8191d",
   "metadata": {},
   "source": [
    "## Streaming Multiple Results\n",
    "\n",
    "Here is an example pattern if you want to stream your multiple results. Note that this is not supported for Hugging Face text completions at this time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a37702",
   "metadata": {},
   "outputs": [],
   "source": [
    "if selectedService == Service.OpenAI:\n",
    "    import os\n",
    "    from IPython.display import clear_output\n",
    "    import time\n",
    "\n",
    "    # Determine the clear command based on OS\n",
    "    clear_command = \"cls\" if os.name == \"nt\" else \"clear\"\n",
    "\n",
    "    chat = ChatHistory()\n",
    "    chat.add_user_message(\"what is the purpose of a rubber duck?\")\n",
    "\n",
    "    stream = oai_text_service.complete_stream(chat_history=chat, settings=oai_text_prompt_execution_settings)\n",
    "    number_of_responses = oai_text_prompt_execution_settings.number_of_responses\n",
    "    texts = [\"\"] * number_of_responses\n",
    "\n",
    "    last_clear_time = time.time()\n",
    "    clear_interval = 0.5  # seconds\n",
    "\n",
    "    # Note: there are some quirks with displaying the output, which sometimes flashes and disappears.\n",
    "    # This could be influenced by a few factors specific to Jupyter notebooks and asynchronous processing.\n",
    "    # The following code attempts to buffer the results to avoid the output flashing on/off the screen.\n",
    "\n",
    "    async for results in stream:\n",
    "        current_time = time.time()\n",
    "\n",
    "        # Update texts with new results\n",
    "        for idx, result in enumerate(results):\n",
    "            if idx < number_of_responses:\n",
    "                texts[idx] += str(result)\n",
    "\n",
    "        # Clear and display output at intervals\n",
    "        if current_time - last_clear_time > clear_interval:\n",
    "            clear_output(wait=True)\n",
    "            for idx, text in enumerate(texts):\n",
    "                print(f\"Result {idx + 1}: {text}\")\n",
    "            last_clear_time = current_time\n",
    "\n",
    "    print(\"----------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< head
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
=======
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
<<<<<<< main
=======
   "version": "3.10.12"
>>>>>>> origin/main
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
   "version": "3.10.12"
>>>>>>> origin/main
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
