{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b10a579",
   "metadata": {},
   "source": [
    "# Semantic Kernel Tutorial\n",
    "\n",
    "This notebook demonstrates the key features of Semantic Kernel with Ollama.\n",
    "\n",
    "## Prerequisites\n",
    "- Ollama running locally on port 11434\n",
    "- llama3.2 model pulled\n",
    "- semantic-kernel Python package installed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a81bf5b",
   "metadata": {},
   "source": [
    "## 1. Basic Setup\n",
    "\n",
    "First, let's import the necessary modules and create a kernel with Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d97b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.ollama import OllamaChatCompletion\n",
    "from semantic_kernel.contents import ChatHistory\n",
    "\n",
    "# Create kernel and add Ollama service\n",
    "kernel = Kernel()\n",
    "service = OllamaChatCompletion(\n",
    "    ai_model_id=\"llama3.2\",\n",
    "    host=\"http://localhost:11434\",\n",
    ")\n",
    "kernel.add_service(service)\n",
    "\n",
    "print(\"✅ Kernel initialized with Ollama service\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e33561",
   "metadata": {},
   "source": [
    "## 2. Simple Chat Completion\n",
    "\n",
    "Let's send a simple message and get a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9114484",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = ChatHistory()\n",
    "chat_history.add_user_message(\"Explain what Semantic Kernel is in one sentence.\")\n",
    "\n",
    "response = await service.get_chat_message_contents(\n",
    "    chat_history=chat_history,\n",
    "    settings=service.get_prompt_execution_settings_class()(\n",
    "        max_tokens=100,\n",
    "        temperature=0.7\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"Response: {response[0].content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0808b4d7",
   "metadata": {},
   "source": [
    "## 3. Creating a Custom Plugin\n",
    "\n",
    "Plugins allow you to extend the AI's capabilities with custom functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f737e7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from semantic_kernel.functions import kernel_function\n",
    "\n",
    "class CalculatorPlugin:\n",
    "    \"\"\"A simple calculator plugin.\"\"\"\n",
    "    \n",
    "    @kernel_function(\n",
    "        name=\"add\",\n",
    "        description=\"Adds two numbers together\"\n",
    "    )\n",
    "    def add(\n",
    "        self,\n",
    "        a: Annotated[float, \"First number\"],\n",
    "        b: Annotated[float, \"Second number\"]\n",
    "    ) -> Annotated[float, \"The sum\"]:\n",
    "        return a + b\n",
    "    \n",
    "    @kernel_function(\n",
    "        name=\"multiply\",\n",
    "        description=\"Multiplies two numbers\"\n",
    "    )\n",
    "    def multiply(\n",
    "        self,\n",
    "        a: Annotated[float, \"First number\"],\n",
    "        b: Annotated[float, \"Second number\"]\n",
    "    ) -> Annotated[float, \"The product\"]:\n",
    "        return a * b\n",
    "\n",
    "# Add plugin to kernel\n",
    "kernel.add_plugin(CalculatorPlugin(), plugin_name=\"calculator\")\n",
    "print(\"✅ Calculator plugin added\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303a0101",
   "metadata": {},
   "source": [
    "## 4. Using Agents with Function Calling\n",
    "\n",
    "Agents can automatically call functions when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d84bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.agents import ChatCompletionAgent\n",
    "from semantic_kernel.connectors.ai import FunctionChoiceBehavior\n",
    "\n",
    "# Create agent with function calling\n",
    "agent = ChatCompletionAgent(\n",
    "    kernel=kernel,\n",
    "    name=\"MathBot\",\n",
    "    instructions=\"You are a math assistant. Use the calculator functions when needed.\",\n",
    "    function_choice_behavior=FunctionChoiceBehavior.Auto(),\n",
    ")\n",
    "\n",
    "print(\"✅ Agent created with function calling enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f37f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agent\n",
    "question = \"What is 25 multiplied by 4, then add 10?\"\n",
    "print(f\"Question: {question}\\n\")\n",
    "\n",
    "response = await agent.get_response(messages=question)\n",
    "print(f\"Answer: {response.message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86c8eca",
   "metadata": {},
   "source": [
    "## 5. Multi-turn Conversation\n",
    "\n",
    "Maintain context across multiple exchanges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e9adec",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatHistory()\n",
    "\n",
    "# Turn 1\n",
    "chat.add_user_message(\"I'm planning a trip to Paris.\")\n",
    "response1 = await service.get_chat_message_contents(\n",
    "    chat_history=chat,\n",
    "    settings=service.get_prompt_execution_settings_class()(max_tokens=80)\n",
    ")\n",
    "chat.add_assistant_message(response1[0].content)\n",
    "print(f\"User: I'm planning a trip to Paris.\")\n",
    "print(f\"AI: {response1[0].content}\\n\")\n",
    "\n",
    "# Turn 2\n",
    "chat.add_user_message(\"What city was I talking about?\")\n",
    "response2 = await service.get_chat_message_contents(\n",
    "    chat_history=chat,\n",
    "    settings=service.get_prompt_execution_settings_class()(max_tokens=50)\n",
    ")\n",
    "print(f\"User: What city was I talking about?\")\n",
    "print(f\"AI: {response2[0].content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c61853",
   "metadata": {},
   "source": [
    "## 6. Streaming Responses\n",
    "\n",
    "Get responses as they're generated for a better user experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ae86cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "import asyncio\n",
    "\n",
    "chat_history = ChatHistory()\n",
    "chat_history.add_user_message(\"Write a short poem about AI.\")\n",
    "\n",
    "print(\"Streaming response:\\n\")\n",
    "full_response = \"\"\n",
    "\n",
    "async for chunk in service.get_streaming_chat_message_contents(\n",
    "    chat_history=chat_history,\n",
    "    settings=service.get_prompt_execution_settings_class()(max_tokens=150)\n",
    "):\n",
    "    if chunk:\n",
    "        for content in chunk:\n",
    "            if content.content:\n",
    "                full_response += content.content\n",
    "                print(content.content, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\\nDone!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638ac01d",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've learned:\n",
    "- ✅ How to set up Semantic Kernel with Ollama\n",
    "- ✅ How to send chat completions\n",
    "- ✅ How to create custom plugins\n",
    "- ✅ How to use agents with function calling\n",
    "- ✅ How to maintain conversation context\n",
    "- ✅ How to stream responses\n",
    "\n",
    "Explore more at: https://github.com/microsoft/semantic-kernel"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
