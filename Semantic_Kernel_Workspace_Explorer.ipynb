{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a01b95e1",
   "metadata": {},
   "source": [
    "# ğŸš€ Semantic Kernel Workspace Explorer\n",
    "\n",
    "**Comprehensive Guide to Your AI Development Environment**\n",
    "\n",
    "Welcome to your complete Semantic Kernel workspace! This notebook will help you:\n",
    "\n",
    "- ğŸ” **Explore** the workspace structure\n",
    "- ğŸ—ï¸ **Set up** your development environment\n",
    "- ğŸ““ **Work with** Jupyter notebooks for AI experiments\n",
    "- ğŸ§ª **Run tests** and builds\n",
    "- ğŸš€ **Deploy** services using Docker\n",
    "- ğŸ“Š **Analyze** the codebase\n",
    "- ğŸ¤– **Work with** AI models and training\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0969a61e",
   "metadata": {},
   "source": [
    "## ğŸ” 1. Workspace Structure Exploration\n",
    "\n",
    "Let's start by understanding what's in this comprehensive workspace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87f6148",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Define workspace root\n",
    "workspace_root = Path(\"/workspaces/semantic-kernel\")\n",
    "print(f\"ğŸ“ Workspace Root: {workspace_root}\")\n",
    "print(f\"âœ… Exists: {workspace_root.exists()}\")\n",
    "print()\n",
    "\n",
    "# Explore main directories\n",
    "main_dirs = [\n",
    "    \"01-core-implementations\",\n",
    "    \"02-ai-workspace\", \n",
    "    \"03-development-tools\",\n",
    "    \"04-infrastructure\",\n",
    "    \"09-agi-development\",\n",
    "    \"13-testing\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ“‚ Main Directory Structure:\")\n",
    "for dir_name in main_dirs:\n",
    "    dir_path = workspace_root / dir_name\n",
    "    if dir_path.exists():\n",
    "        print(f\"  âœ… {dir_name}/\")\n",
    "        # Show subdirectories\n",
    "        subdirs = [d.name for d in dir_path.iterdir() if d.is_dir()][:5]\n",
    "        if subdirs:\n",
    "            print(f\"     â””â”€â”€ {', '.join(subdirs)}\")\n",
    "    else:\n",
    "        print(f\"  âŒ {dir_name}/ (not found)\")\n",
    "\n",
    "print(\"\\nğŸ”§ Key Files:\")\n",
    "key_files = [\n",
    "    \"unified_launcher.py\",\n",
    "    \"run.py\", \n",
    "    \"README.md\",\n",
    "    \"docker-compose.yml\"\n",
    "]\n",
    "\n",
    "for file_name in key_files:\n",
    "    file_path = workspace_root / file_name\n",
    "    print(f\"  {'âœ…' if file_path.exists() else 'âŒ'} {file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66adc9ce",
   "metadata": {},
   "source": [
    "## ğŸ› ï¸ 2. Available Programming Languages & Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410403ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available language implementations\n",
    "core_impl_path = workspace_root / \"01-core-implementations\"\n",
    "\n",
    "if core_impl_path.exists():\n",
    "    print(\"ğŸŒ Available Language Implementations:\")\n",
    "    for lang_dir in core_impl_path.iterdir():\n",
    "        if lang_dir.is_dir():\n",
    "            print(f\"  ğŸ“š {lang_dir.name}\")\n",
    "            \n",
    "            # Check for README or key files\n",
    "            readme = lang_dir / \"README.md\"\n",
    "            if readme.exists():\n",
    "                print(f\"     ğŸ“– Has documentation\")\n",
    "            \n",
    "            # Language-specific checks\n",
    "            if lang_dir.name == \"python\":\n",
    "                pyproject = lang_dir / \"pyproject.toml\"\n",
    "                requirements = lang_dir / \"requirements.txt\"\n",
    "                print(f\"     ğŸ pyproject.toml: {'âœ…' if pyproject.exists() else 'âŒ'}\")\n",
    "                print(f\"     ğŸ“¦ requirements.txt: {'âœ…' if requirements.exists() else 'âŒ'}\")\n",
    "            \n",
    "            elif lang_dir.name == \"dotnet\":\n",
    "                sln_files = list(lang_dir.glob(\"*.sln\"))\n",
    "                print(f\"     ğŸ”· .NET solution files: {len(sln_files)}\")\n",
    "            \n",
    "            elif lang_dir.name == \"java\":\n",
    "                pom_files = list(lang_dir.glob(\"**/pom.xml\"))\n",
    "                print(f\"     â˜• Maven pom.xml files: {len(pom_files)}\")\n",
    "else:\n",
    "    print(\"âŒ Core implementations directory not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29283498",
   "metadata": {},
   "source": [
    "## ğŸ—ï¸ 3. Development Environment Setup\n",
    "\n",
    "Let's check your current environment and set up everything you need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900e9c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import shutil\n",
    "\n",
    "print(\"ğŸ” Current Environment Check:\")\n",
    "print(f\"  ğŸ Python: {sys.version}\")\n",
    "print(f\"  ğŸ“ Python Path: {sys.executable}\")\n",
    "print(f\"  ğŸ“ Working Directory: {os.getcwd()}\")\n",
    "print()\n",
    "\n",
    "# Check for essential tools\n",
    "tools = {\n",
    "    \"git\": \"Git version control\",\n",
    "    \"docker\": \"Docker containerization\", \n",
    "    \"node\": \"Node.js runtime\",\n",
    "    \"dotnet\": \".NET CLI\",\n",
    "    \"java\": \"Java runtime\",\n",
    "    \"mvn\": \"Maven build tool\"\n",
    "}\n",
    "\n",
    "print(\"ğŸ› ï¸ Available Tools:\")\n",
    "for tool, description in tools.items():\n",
    "    tool_path = shutil.which(tool)\n",
    "    if tool_path:\n",
    "        try:\n",
    "            result = subprocess.run([tool, \"--version\"], capture_output=True, text=True, timeout=5)\n",
    "            version = result.stdout.split('\\n')[0][:50]\n",
    "            print(f\"  âœ… {tool}: {version}\")\n",
    "        except:\n",
    "            print(f\"  âœ… {tool}: Available at {tool_path}\")\n",
    "    else:\n",
    "        print(f\"  âŒ {tool}: Not found\")\n",
    "\n",
    "print(\"\\nğŸ“¦ Python Packages Check:\")\n",
    "required_packages = [\n",
    "    \"semantic-kernel\",\n",
    "    \"openai\", \n",
    "    \"azure-ai-search\",\n",
    "    \"jupyter\",\n",
    "    \"numpy\",\n",
    "    \"pandas\"\n",
    "]\n",
    "\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        __import__(package.replace('-', '_'))\n",
    "        print(f\"  âœ… {package}\")\n",
    "    except ImportError:\n",
    "        print(f\"  âŒ {package} (needs installation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f502e6",
   "metadata": {},
   "source": [
    "### ğŸ”§ Install Missing Dependencies\n",
    "\n",
    "Run this cell to install essential packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6efccdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install essential packages\n",
    "essential_packages = [\n",
    "    \"semantic-kernel\",\n",
    "    \"openai\",\n",
    "    \"jupyter\", \n",
    "    \"ipykernel\",\n",
    "    \"numpy\",\n",
    "    \"pandas\",\n",
    "    \"matplotlib\",\n",
    "    \"requests\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ“¦ Installing essential packages...\")\n",
    "for package in essential_packages:\n",
    "    try:\n",
    "        result = subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", package], \n",
    "                              capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(f\"  âœ… {package} installed successfully\")\n",
    "        else:\n",
    "            print(f\"  âš ï¸ {package} installation had issues\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ Failed to install {package}: {e}\")\n",
    "\n",
    "print(\"\\nğŸ‰ Installation complete! Please restart the kernel if needed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0ba20e",
   "metadata": {},
   "source": [
    "## ğŸ““ 4. Jupyter Notebooks & AI Experiments\n",
    "\n",
    "Let's explore the available notebooks and create some AI experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934bb0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all Jupyter notebooks in the workspace\n",
    "notebooks = list(workspace_root.glob(\"**/*.ipynb\"))\n",
    "\n",
    "print(f\"ğŸ““ Found {len(notebooks)} Jupyter Notebooks:\")\n",
    "for notebook in notebooks[:10]:  # Show first 10\n",
    "    rel_path = notebook.relative_to(workspace_root)\n",
    "    print(f\"  ğŸ“„ {rel_path}\")\n",
    "\n",
    "if len(notebooks) > 10:\n",
    "    print(f\"  ... and {len(notebooks) - 10} more\")\n",
    "\n",
    "# Check AI workspace notebooks specifically\n",
    "ai_notebooks_path = workspace_root / \"02-ai-workspace\" / \"01-notebooks\"\n",
    "if ai_notebooks_path.exists():\n",
    "    print(f\"\\nğŸ¤– AI Workspace Notebooks:\")\n",
    "    for notebook in ai_notebooks_path.glob(\"*.ipynb\"):\n",
    "        print(f\"  ğŸš€ {notebook.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285cee1d",
   "metadata": {},
   "source": [
    "### ğŸ§ª Quick AI Experiment Setup\n",
    "\n",
    "Let's set up a basic Semantic Kernel experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52400a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Semantic Kernel setup and demo\n",
    "try:\n",
    "    import semantic_kernel as sk\n",
    "    print(\"âœ… Semantic Kernel imported successfully!\")\n",
    "    \n",
    "    # Create a kernel instance\n",
    "    kernel = sk.Kernel()\n",
    "    print(\"ğŸ”§ Kernel created\")\n",
    "    \n",
    "    # Basic functionality test\n",
    "    print(\"\\nğŸ§ª Basic Kernel Test:\")\n",
    "    print(f\"  ğŸ“ Kernel type: {type(kernel)}\")\n",
    "    \n",
    "    # Check available plugins/connectors\n",
    "    print(\"\\nğŸ”Œ Available Core Plugins:\")\n",
    "    try:\n",
    "        import semantic_kernel.core_plugins as core_plugins\n",
    "        plugins = [attr for attr in dir(core_plugins) if not attr.startswith('_')]\n",
    "        for plugin in plugins[:5]:\n",
    "            print(f\"  ğŸ”§ {plugin}\")\n",
    "    except:\n",
    "        print(\"  â„¹ï¸ Core plugins structure may have changed\")\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Semantic Kernel not available: {e}\")\n",
    "    print(\"ğŸ’¡ Run the installation cell above to install semantic-kernel\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Error setting up Semantic Kernel: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de65de8b",
   "metadata": {},
   "source": [
    "## ğŸ§ª 5. Running Tests & Builds\n",
    "\n",
    "Let's explore the testing infrastructure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed70c4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find test files and build scripts\n",
    "test_files = list(workspace_root.glob(\"**/test_*.py\"))\n",
    "print(f\"ğŸ§ª Found {len(test_files)} Python test files:\")\n",
    "for test_file in test_files[:10]:\n",
    "    rel_path = test_file.relative_to(workspace_root)\n",
    "    print(f\"  ğŸ”¬ {rel_path}\")\n",
    "\n",
    "# Check for build files\n",
    "build_files = [\n",
    "    \"Makefile\",\n",
    "    \"build.sh\", \n",
    "    \"setup.py\",\n",
    "    \"pyproject.toml\"\n",
    "]\n",
    "\n",
    "print(f\"\\nğŸ—ï¸ Build Configuration Files:\")\n",
    "for build_file in build_files:\n",
    "    matches = list(workspace_root.glob(f\"**/{build_file}\"))\n",
    "    if matches:\n",
    "        print(f\"  âœ… {build_file}: {len(matches)} found\")\n",
    "        for match in matches[:3]:\n",
    "            rel_path = match.relative_to(workspace_root)\n",
    "            print(f\"     ğŸ“ {rel_path}\")\n",
    "    else:\n",
    "        print(f\"  âŒ {build_file}: Not found\")\n",
    "\n",
    "# Check testing directory\n",
    "testing_dir = workspace_root / \"13-testing\"\n",
    "if testing_dir.exists():\n",
    "    print(f\"\\nğŸ¯ Testing Directory Contents:\")\n",
    "    for item in testing_dir.iterdir():\n",
    "        print(f\"  {'ğŸ“' if item.is_dir() else 'ğŸ“„'} {item.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7619bdb3",
   "metadata": {},
   "source": [
    "### ğŸš€ Quick Test Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf2bf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a simple test to verify the workspace\n",
    "def quick_workspace_test():\n",
    "    \"\"\"Run a quick test of workspace functionality\"\"\"\n",
    "    tests_passed = 0\n",
    "    total_tests = 0\n",
    "    \n",
    "    print(\"ğŸƒâ€â™‚ï¸ Running Quick Workspace Tests...\\n\")\n",
    "    \n",
    "    # Test 1: Workspace structure\n",
    "    total_tests += 1\n",
    "    print(\"Test 1: Workspace Structure\")\n",
    "    if workspace_root.exists():\n",
    "        print(\"  âœ… Workspace root exists\")\n",
    "        tests_passed += 1\n",
    "    else:\n",
    "        print(\"  âŒ Workspace root missing\")\n",
    "    \n",
    "    # Test 2: Python environment\n",
    "    total_tests += 1\n",
    "    print(\"\\nTest 2: Python Environment\")\n",
    "    if sys.version_info >= (3, 8):\n",
    "        print(f\"  âœ… Python version OK: {sys.version_info.major}.{sys.version_info.minor}\")\n",
    "        tests_passed += 1\n",
    "    else:\n",
    "        print(f\"  âŒ Python version too old: {sys.version_info.major}.{sys.version_info.minor}\")\n",
    "    \n",
    "    # Test 3: Key directories\n",
    "    total_tests += 1\n",
    "    print(\"\\nTest 3: Key Directories\")\n",
    "    key_dirs = [\"01-core-implementations\", \"02-ai-workspace\"]\n",
    "    missing_dirs = [d for d in key_dirs if not (workspace_root / d).exists()]\n",
    "    if not missing_dirs:\n",
    "        print(\"  âœ… All key directories present\")\n",
    "        tests_passed += 1\n",
    "    else:\n",
    "        print(f\"  âŒ Missing directories: {missing_dirs}\")\n",
    "    \n",
    "    # Test 4: File operations\n",
    "    total_tests += 1\n",
    "    print(\"\\nTest 4: File Operations\")\n",
    "    try:\n",
    "        test_file = workspace_root / \"test_workspace.tmp\"\n",
    "        test_file.write_text(\"test\")\n",
    "        content = test_file.read_text()\n",
    "        test_file.unlink()\n",
    "        if content == \"test\":\n",
    "            print(\"  âœ… File operations working\")\n",
    "            tests_passed += 1\n",
    "        else:\n",
    "            print(\"  âŒ File operations failed\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ File operations error: {e}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Test Results: {tests_passed}/{total_tests} passed\")\n",
    "    if tests_passed == total_tests:\n",
    "        print(\"ğŸ‰ All tests passed! Workspace is ready.\")\n",
    "    else:\n",
    "        print(\"âš ï¸ Some tests failed. Check the setup.\")\n",
    "    \n",
    "    return tests_passed, total_tests\n",
    "\n",
    "# Run the test\n",
    "passed, total = quick_workspace_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f39ca3",
   "metadata": {},
   "source": [
    "## ğŸš€ 6. Docker Deployment System\n",
    "\n",
    "Let's explore the deployment infrastructure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1311c8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Docker-related files\n",
    "docker_files = {\n",
    "    \"Dockerfile\": \"Main Docker image definition\",\n",
    "    \"docker-compose.yml\": \"Multi-service deployment\",\n",
    "    \"docker-compose.dev.yml\": \"Development environment\",\n",
    "    \".dockerignore\": \"Docker ignore patterns\"\n",
    "}\n",
    "\n",
    "print(\"ğŸ³ Docker Configuration Files:\")\n",
    "for filename, description in docker_files.items():\n",
    "    matches = list(workspace_root.glob(f\"**/{filename}\"))\n",
    "    if matches:\n",
    "        print(f\"  âœ… {filename}: {len(matches)} found\")\n",
    "        for match in matches[:2]:\n",
    "            rel_path = match.relative_to(workspace_root)\n",
    "            print(f\"     ğŸ“ {rel_path}\")\n",
    "    else:\n",
    "        print(f\"  âŒ {filename}: Not found\")\n",
    "\n",
    "# Check deployment scripts\n",
    "deployment_scripts = list(workspace_root.glob(\"**/deploy*.py\")) + \\\n",
    "                    list(workspace_root.glob(\"**/deploy*.sh\"))\n",
    "\n",
    "print(f\"\\nğŸš€ Deployment Scripts ({len(deployment_scripts)} found):\")\n",
    "for script in deployment_scripts[:5]:\n",
    "    rel_path = script.relative_to(workspace_root)\n",
    "    print(f\"  ğŸ”§ {rel_path}\")\n",
    "\n",
    "# Check if Docker is available\n",
    "print(\"\\nğŸ” Docker Availability:\")\n",
    "docker_available = shutil.which(\"docker\") is not None\n",
    "print(f\"  Docker CLI: {'âœ… Available' if docker_available else 'âŒ Not found'}\")\n",
    "\n",
    "if docker_available:\n",
    "    try:\n",
    "        result = subprocess.run([\"docker\", \"--version\"], capture_output=True, text=True, timeout=5)\n",
    "        if result.returncode == 0:\n",
    "            print(f\"  Version: {result.stdout.strip()}\")\n",
    "    except:\n",
    "        print(\"  âš ï¸ Docker command failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7cef29",
   "metadata": {},
   "source": [
    "## ğŸ“Š 7. Codebase Analysis\n",
    "\n",
    "Let's analyze the structure and content of the codebase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ab51fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze file types and sizes\n",
    "file_stats = {\n",
    "    \".py\": {\"count\": 0, \"size\": 0, \"name\": \"Python\"},\n",
    "    \".cs\": {\"count\": 0, \"size\": 0, \"name\": \"C#\"},\n",
    "    \".java\": {\"count\": 0, \"size\": 0, \"name\": \"Java\"},\n",
    "    \".ts\": {\"count\": 0, \"size\": 0, \"name\": \"TypeScript\"},\n",
    "    \".js\": {\"count\": 0, \"size\": 0, \"name\": \"JavaScript\"},\n",
    "    \".md\": {\"count\": 0, \"size\": 0, \"name\": \"Markdown\"},\n",
    "    \".json\": {\"count\": 0, \"size\": 0, \"name\": \"JSON\"},\n",
    "    \".yml\": {\"count\": 0, \"size\": 0, \"name\": \"YAML\"},\n",
    "    \".yaml\": {\"count\": 0, \"size\": 0, \"name\": \"YAML\"}\n",
    "}\n",
    "\n",
    "print(\"ğŸ“Š Analyzing codebase...\")\n",
    "\n",
    "# Count files (limit to avoid performance issues)\n",
    "analyzed_files = 0\n",
    "max_files = 1000\n",
    "\n",
    "for file_path in workspace_root.rglob(\"*\"):\n",
    "    if analyzed_files >= max_files:\n",
    "        break\n",
    "        \n",
    "    if file_path.is_file():\n",
    "        analyzed_files += 1\n",
    "        suffix = file_path.suffix.lower()\n",
    "        \n",
    "        if suffix in file_stats:\n",
    "            try:\n",
    "                size = file_path.stat().st_size\n",
    "                file_stats[suffix][\"count\"] += 1\n",
    "                file_stats[suffix][\"size\"] += size\n",
    "            except:\n",
    "                pass  # Skip files we can't access\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Codebase Statistics (analyzed {analyzed_files} files):\")\n",
    "print(f\"{'Language':<12} {'Files':<8} {'Total Size':<12}\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "for ext, stats in file_stats.items():\n",
    "    if stats[\"count\"] > 0:\n",
    "        size_mb = stats[\"size\"] / (1024 * 1024)\n",
    "        print(f\"{stats['name']:<12} {stats['count']:<8} {size_mb:.2f} MB\")\n",
    "\n",
    "# Find largest files\n",
    "print(\"\\nğŸ“ Largest Files:\")\n",
    "large_files = []\n",
    "checked_files = 0\n",
    "max_check = 200\n",
    "\n",
    "for file_path in workspace_root.rglob(\"*\"):\n",
    "    if checked_files >= max_check:\n",
    "        break\n",
    "    if file_path.is_file():\n",
    "        checked_files += 1\n",
    "        try:\n",
    "            size = file_path.stat().st_size\n",
    "            if size > 100000:  # > 100KB\n",
    "                large_files.append((file_path, size))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "large_files.sort(key=lambda x: x[1], reverse=True)\n",
    "for file_path, size in large_files[:5]:\n",
    "    rel_path = file_path.relative_to(workspace_root)\n",
    "    size_mb = size / (1024 * 1024)\n",
    "    print(f\"  ğŸ“„ {rel_path} ({size_mb:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea6c38c",
   "metadata": {},
   "source": [
    "## ğŸ¤– 8. AI Models & Training\n",
    "\n",
    "Let's explore the AI model capabilities and training infrastructure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c4a84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find AI model related files\n",
    "model_patterns = [\n",
    "    \"**/models/**\",\n",
    "    \"**/*model*\",\n",
    "    \"**/*training*\", \n",
    "    \"**/*finetune*\",\n",
    "    \"**/huggingface/**\",\n",
    "    \"**/*llm*\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ¤– AI Model & Training Files:\")\n",
    "ai_files = set()\n",
    "\n",
    "for pattern in model_patterns:\n",
    "    matches = list(workspace_root.glob(pattern))\n",
    "    for match in matches:\n",
    "        if match.is_file() and match.suffix in ['.py', '.md', '.json', '.yml']:\n",
    "            ai_files.add(match)\n",
    "\n",
    "# Group by directory\n",
    "ai_dirs = {}\n",
    "for file_path in list(ai_files)[:20]:  # Limit display\n",
    "    parent = file_path.parent\n",
    "    rel_parent = parent.relative_to(workspace_root)\n",
    "    if rel_parent not in ai_dirs:\n",
    "        ai_dirs[rel_parent] = []\n",
    "    ai_dirs[rel_parent].append(file_path.name)\n",
    "\n",
    "for dir_path, files in list(ai_dirs.items())[:10]:\n",
    "    print(f\"\\nğŸ“ {dir_path}:\")\n",
    "    for file_name in files[:5]:\n",
    "        print(f\"  ğŸ“„ {file_name}\")\n",
    "    if len(files) > 5:\n",
    "        print(f\"  ... and {len(files) - 5} more\")\n",
    "\n",
    "# Check for specific AI training directories\n",
    "ai_training_dirs = [\n",
    "    \"02-ai-workspace/03-models-training\",\n",
    "    \"09-agi-development\", \n",
    "    \"19-miscellaneous/llm\"\n",
    "]\n",
    "\n",
    "print(f\"\\nğŸ¯ Dedicated AI Training Directories:\")\n",
    "for dir_name in ai_training_dirs:\n",
    "    dir_path = workspace_root / dir_name\n",
    "    if dir_path.exists():\n",
    "        print(f\"  âœ… {dir_name}\")\n",
    "        files = list(dir_path.glob(\"*.py\"))[:3]\n",
    "        for file_path in files:\n",
    "            print(f\"     ğŸ {file_path.name}\")\n",
    "    else:\n",
    "        print(f\"  âŒ {dir_name} (not found)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb37d7f",
   "metadata": {},
   "source": [
    "### ğŸ–¥ï¸ GPU & Hardware Check for AI Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5974b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU availability and AI frameworks\n",
    "print(\"ğŸ–¥ï¸ Hardware & AI Framework Check:\")\n",
    "\n",
    "# Check PyTorch\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"  âœ… PyTorch: {torch.__version__}\")\n",
    "    print(f\"  ğŸ”¥ CUDA Available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"  ğŸ® GPU Count: {torch.cuda.device_count()}\")\n",
    "        print(f\"  ğŸ“± Current Device: {torch.cuda.get_device_name(0)}\")\n",
    "except ImportError:\n",
    "    print(\"  âŒ PyTorch not installed\")\n",
    "\n",
    "# Check TensorFlow\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    print(f\"  âœ… TensorFlow: {tf.__version__}\")\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    print(f\"  ğŸ® TF GPU Devices: {len(gpus)}\")\n",
    "except ImportError:\n",
    "    print(\"  âŒ TensorFlow not installed\")\n",
    "\n",
    "# Check transformers\n",
    "try:\n",
    "    import transformers\n",
    "    print(f\"  âœ… Transformers: {transformers.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"  âŒ Transformers not installed\")\n",
    "\n",
    "# System resources\n",
    "try:\n",
    "    import psutil\n",
    "    print(f\"\\nğŸ’¾ System Resources:\")\n",
    "    print(f\"  ğŸ§  CPU Cores: {psutil.cpu_count()}\")\n",
    "    memory = psutil.virtual_memory()\n",
    "    print(f\"  ğŸ’¾ RAM: {memory.total / (1024**3):.1f} GB total, {memory.available / (1024**3):.1f} GB available\")\n",
    "    disk = psutil.disk_usage('/')\n",
    "    print(f\"  ğŸ’½ Disk: {disk.total / (1024**3):.1f} GB total, {disk.free / (1024**3):.1f} GB free\")\n",
    "except ImportError:\n",
    "    print(\"\\nğŸ’¾ System Resources: (psutil not available)\")\n",
    "    print(\"  â„¹ï¸ Install psutil for detailed system info\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f7d606",
   "metadata": {},
   "source": [
    "## ğŸ‰ 9. Workspace Summary & Next Steps\n",
    "\n",
    "Here's what we discovered about your Semantic Kernel workspace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a88a5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a comprehensive summary\n",
    "print(\"ğŸ¯ SEMANTIC KERNEL WORKSPACE SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Workspace status\n",
    "workspace_status = \"âœ… Ready\" if workspace_root.exists() else \"âŒ Issues Found\"\n",
    "print(f\"ğŸ“ Workspace Status: {workspace_status}\")\n",
    "print(f\"ğŸ“ Location: {workspace_root}\")\n",
    "\n",
    "# Count main components\n",
    "components = {\n",
    "    \"Python files\": len(list(workspace_root.glob(\"**/*.py\"))),\n",
    "    \"Jupyter notebooks\": len(list(workspace_root.glob(\"**/*.ipynb\"))),\n",
    "    \"Markdown docs\": len(list(workspace_root.glob(\"**/*.md\"))),\n",
    "    \"Docker files\": len(list(workspace_root.glob(\"**/Dockerfile\"))) + len(list(workspace_root.glob(\"**/docker-compose*.yml\")))\n",
    "}\n",
    "\n",
    "print(f\"\\nğŸ“Š Workspace Components:\")\n",
    "for component, count in components.items():\n",
    "    print(f\"  {component}: {count}\")\n",
    "\n",
    "# Recommendations\n",
    "print(f\"\\nğŸš€ RECOMMENDED NEXT STEPS:\")\n",
    "print(f\"\")\n",
    "print(f\"1. ğŸ—ï¸ SETUP: Run the installation cells above if any packages were missing\")\n",
    "print(f\"2. ğŸ““ NOTEBOOKS: Explore the AI workspace notebooks in 02-ai-workspace/01-notebooks/\")\n",
    "print(f\"3. ğŸ§ª TESTING: Run the unified launcher: python unified_launcher.py\")\n",
    "print(f\"4. ğŸš€ DEPLOY: Try the Docker deployment: docker-compose up -d\")\n",
    "print(f\"5. ğŸ¤– AI MODELS: Check out the AGI development notebooks in 09-agi-development/\")\n",
    "print(f\"\")\n",
    "print(f\"ğŸ’¡ QUICK COMMANDS:\")\n",
    "print(f\"  # Navigate to AI workspace\")\n",
    "print(f\"  cd /workspaces/semantic-kernel/02-ai-workspace\")\n",
    "print(f\"  \")\n",
    "print(f\"  # Launch interactive control\")\n",
    "print(f\"  python ai_workspace_control.py --interactive\")\n",
    "print(f\"  \")\n",
    "print(f\"  # Start Jupyter Lab\")\n",
    "print(f\"  jupyter lab\")\n",
    "print(f\"\")\n",
    "print(f\"ğŸ‰ Your Semantic Kernel workspace is comprehensive and ready for AI development!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd2a94b",
   "metadata": {},
   "source": [
    "## ğŸ“š 10. Additional Resources & Documentation\n",
    "\n",
    "### ğŸ”— Key Documentation Files:\n",
    "- `README.md` - Main project documentation\n",
    "- `02-ai-workspace/README.md` - AI workspace guide\n",
    "- `01-core-implementations/python/DEV_SETUP.md` - Python development setup\n",
    "- `UNIFIED_LAUNCHER_README.md` - Unified launcher guide\n",
    "\n",
    "### ğŸ› ï¸ Key Tools:\n",
    "- `unified_launcher.py` - Main workspace launcher\n",
    "- `ai_workspace_control.py` - AI workspace management\n",
    "- `organize_files.py` - File organization utility\n",
    "- `setup_local_ai.py` - Local AI environment setup\n",
    "\n",
    "### ğŸ¯ Getting Started Paths:\n",
    "\n",
    "**For AI Research:**\n",
    "1. Start with notebooks in `02-ai-workspace/01-notebooks/`\n",
    "2. Explore AGI development in `09-agi-development/`\n",
    "3. Try model training in `02-ai-workspace/03-models-training/`\n",
    "\n",
    "**For Development:**\n",
    "1. Check language implementations in `01-core-implementations/`\n",
    "2. Run tests from `13-testing/`\n",
    "3. Use deployment tools in `02-ai-workspace/09-deployment/`\n",
    "\n",
    "**For Production:**\n",
    "1. Use Docker deployment system\n",
    "2. Configure environment variables\n",
    "3. Set up CI/CD pipelines from `04-infrastructure/`\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ‰ Congratulations! You now have a complete overview of your Semantic Kernel workspace. Choose any path above and start building amazing AI applications!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
